{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# üöÄ RAG Document Ingestion for LOCAL Development\n",
    "\n",
    "Pipeline: PDF (local storage) ‚Üí Text ‚Üí Chunks ‚Üí Embeddings ‚Üí PostgreSQL\n",
    "\n",
    "**Note:** Embeddings stored as TEXT (JSON array) since pgvector unavailable in Alpine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup - Database & Storage Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Testing Docker PostgreSQL connection...\n",
      "‚úÖ Connected to database\n",
      "‚úÖ PostgreSQL: PostgreSQL 15.15 on x86_64-pc-linux-musl\n",
      "‚ö†Ô∏è  SQLAlchemy direct connection failed: (psycopg2.ProgrammingError) invalid dsn: invalid connection option \"timeout\"\n",
      "\n",
      "(Background on this er\n",
      "   Will use Docker subprocess method instead\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sqlalchemy import create_engine, text as sql_text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "# ‚úÖ CONNECTION METHOD: Execute SQL via Docker container\n",
    "# This avoids authentication issues from local machine\n",
    "\n",
    "def execute_sql(query: str, fetch: bool = False):\n",
    "    \"\"\"Execute SQL query via Docker PostgreSQL\"\"\"\n",
    "    cmd = [\n",
    "        \"docker-compose\", \"-f\", \"docker-compose.local.yml\", \"exec\",\n",
    "        \"postgres\", \"psql\", \"-U\", \"llm_user\", \"-d\", \"system_llm\",\n",
    "        \"-t\", \"-c\", query\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, cwd=\".\")\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        raise Exception(f\"SQL Error: {result.stderr}\")\n",
    "    \n",
    "    if fetch:\n",
    "        return result.stdout.strip()\n",
    "    return result.stdout\n",
    "\n",
    "# Test connection\n",
    "print(\"üìå Testing Docker PostgreSQL connection...\")\n",
    "try:\n",
    "    db_version = execute_sql(\"SELECT version();\", fetch=True)\n",
    "    print(f\"‚úÖ Connected to database\")\n",
    "    print(f\"‚úÖ PostgreSQL: {db_version.split(',')[0]}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    raise\n",
    "\n",
    "# Also create SQLAlchemy engine for ORM usage in later cells\n",
    "# (this will try direct connection, but OK if fails - we can use subprocess method)\n",
    "try:\n",
    "    DATABASE_URL = \"postgresql://llm_user:llm_password_local@127.0.0.1:5432/system_llm\"\n",
    "    engine = create_engine(DATABASE_URL, pool_pre_ping=True, pool_size=5, pool_recycle=3600, connect_args={\"timeout\": 5})\n",
    "    SessionLocal = sessionmaker(bind=engine)\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(sql_text(\"SELECT 1\"))\n",
    "    print(\"‚úÖ SQLAlchemy connection also working (bonus!)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  SQLAlchemy direct connection failed: {str(e)[:100]}\")\n",
    "    print(\"   Will use Docker subprocess method instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c86023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup LOCAL file storage\n",
    "from pathlib import Path\n",
    "\n",
    "class LocalFileStorage:\n",
    "    def __init__(self, base_path=\"storage/uploads\"):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.base_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def get(self, file_id: str) -> bytes:\n",
    "        \"\"\"Get file from local storage by file_id (filename without extension)\"\"\"\n",
    "        path = self.base_path / f\"{file_id}.pdf\"\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"File not found: {path}\")\n",
    "        return path.read_bytes()\n",
    "    \n",
    "    def list_files(self):\n",
    "        \"\"\"List all PDF files in storage\"\"\"\n",
    "        return list(self.base_path.glob(\"*.pdf\"))\n",
    "\n",
    "storage = LocalFileStorage()\n",
    "print(f\"‚úÖ Local storage: {storage.base_path.absolute()}\")\n",
    "print(f\"‚úÖ Files available: {len(storage.list_files())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Get Documents from Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Available documents in database:\n",
      "\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xab in position 113: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müìö Available documents in database:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m doc_list = []\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[32m      6\u001b[39m     result = conn.execute(sql_text(\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33m        SELECT id, original_filename, filename, file_size, status \u001b[39m\n\u001b[32m      8\u001b[39m \u001b[33m        FROM document \u001b[39m\n\u001b[32m      9\u001b[39m \u001b[33m        ORDER BY uploaded_at DESC\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\"\"\u001b[39m))\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(result.fetchall(), \u001b[32m1\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pcgsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3273\u001b[39m, in \u001b[36mEngine.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3250\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Connection:\n\u001b[32m   3251\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a new :class:`_engine.Connection` object.\u001b[39;00m\n\u001b[32m   3252\u001b[39m \n\u001b[32m   3253\u001b[39m \u001b[33;03m    The :class:`_engine.Connection` acts as a Python context manager, so\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3270\u001b[39m \n\u001b[32m   3271\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3273\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pcgsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:145\u001b[39m, in \u001b[36mConnection.__init__\u001b[39m\u001b[34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m         \u001b[38;5;28mself\u001b[39m._dbapi_connection = \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraw_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m dialect.loaded_dbapi.Error \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    147\u001b[39m         Connection._handle_dbapi_exception_noconnection(\n\u001b[32m    148\u001b[39m             err, dialect, engine\n\u001b[32m    149\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pcgsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3297\u001b[39m, in \u001b[36mEngine.raw_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3275\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mraw_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> PoolProxiedConnection:\n\u001b[32m   3276\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001b[39;00m\n\u001b[32m   3277\u001b[39m \n\u001b[32m   3278\u001b[39m \u001b[33;03m    The returned object is a proxied version of the DBAPI\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3295\u001b[39m \n\u001b[32m   3296\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pcgsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:449\u001b[39m, in \u001b[36mPool.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> PoolProxiedConnection:\n\u001b[32m    442\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[32m    443\u001b[39m \n\u001b[32m    444\u001b[39m \u001b[33;03m    The connection is instrumented such that when its\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    447\u001b[39m \n\u001b[32m    448\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionFairy\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_checkout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pcgsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:1264\u001b[39m, in \u001b[36m_ConnectionFairy._checkout\u001b[39m\u001b[34m(cls, pool, threadconns, fairy)\u001b[39m\n\u001b[32m   1256\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   1257\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_checkout\u001b[39m(\n\u001b[32m   1258\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1261\u001b[39m     fairy: Optional[_ConnectionFairy] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1262\u001b[39m ) -> _ConnectionFairy:\n\u001b[32m   1263\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fairy:\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m         fairy = \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheckout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1266\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m threadconns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1267\u001b[39m             threadconns.current = weakref.ref(fairy)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pcgsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:713\u001b[39m, in \u001b[36m_ConnectionRecord.checkout\u001b[39m\u001b[34m(cls, pool)\u001b[39m\n\u001b[32m    711\u001b[39m     rec = cast(_ConnectionRecord, pool._do_get())\n\u001b[32m    712\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m     rec = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_do_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    716\u001b[39m     dbapi_connection = rec.get_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pcgsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:179\u001b[39m, in \u001b[36mQueuePool._do_get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_connection()\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dec_overflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pcgsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:224\u001b[39m, in \u001b[36msafe_reraise.__exit__\u001b[39m\u001b[34m(self, type_, value, traceback)\u001b[39m\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m exc_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value.with_traceback(exc_tb)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    226\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pcgsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:177\u001b[39m, in \u001b[36mQueuePool._do_get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inc_overflow():\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m util.safe_reraise():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pcgsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:390\u001b[39m, in \u001b[36mPool._create_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> ConnectionPoolEntry:\n\u001b[32m    388\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pcgsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:675\u001b[39m, in \u001b[36m_ConnectionRecord.__init__\u001b[39m\u001b[34m(self, pool, connect)\u001b[39m\n\u001b[32m    673\u001b[39m \u001b[38;5;28mself\u001b[39m.__pool = pool\n\u001b[32m    674\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connect:\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[38;5;28mself\u001b[39m.finalize_callback = deque()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pcgsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:901\u001b[39m, in \u001b[36m_ConnectionRecord.__connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    899\u001b[39m     \u001b[38;5;28mself\u001b[39m.fresh = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    900\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m901\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mError on connect(): \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    904\u001b[39m     \u001b[38;5;66;03m# in SQLAlchemy 1.4 the first_connect event is not used by\u001b[39;00m\n\u001b[32m    905\u001b[39m     \u001b[38;5;66;03m# the engine, so this will usually not be set\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pcgsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:224\u001b[39m, in \u001b[36msafe_reraise.__exit__\u001b[39m\u001b[34m(self, type_, value, traceback)\u001b[39m\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m exc_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value.with_traceback(exc_tb)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    226\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pcgsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:897\u001b[39m, in \u001b[36m_ConnectionRecord.__connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    895\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    896\u001b[39m     \u001b[38;5;28mself\u001b[39m.starttime = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m     \u001b[38;5;28mself\u001b[39m.dbapi_connection = connection = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_invoke_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    898\u001b[39m     pool.logger.debug(\u001b[33m\"\u001b[39m\u001b[33mCreated new connection \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, connection)\n\u001b[32m    899\u001b[39m     \u001b[38;5;28mself\u001b[39m.fresh = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pcgsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sqlalchemy\\engine\\create.py:646\u001b[39m, in \u001b[36mcreate_engine.<locals>.connect\u001b[39m\u001b[34m(connection_record)\u001b[39m\n\u001b[32m    643\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    644\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m connection\n\u001b[32m--> \u001b[39m\u001b[32m646\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pcgsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:625\u001b[39m, in \u001b[36mDefaultDialect.connect\u001b[39m\u001b[34m(self, *cargs, **cparams)\u001b[39m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m, *cargs: Any, **cparams: Any) -> DBAPIConnection:\n\u001b[32m    624\u001b[39m     \u001b[38;5;66;03m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloaded_dbapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pcgsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\psycopg2\\__init__.py:122\u001b[39m, in \u001b[36mconnect\u001b[39m\u001b[34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     kwasync[\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m] = kwargs.pop(\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    121\u001b[39m dsn = _ext.make_dsn(dsn, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m conn = \u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwasync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    124\u001b[39m     conn.cursor_factory = cursor_factory\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0xab in position 113: invalid start byte"
     ]
    }
   ],
   "source": [
    "# Get all documents\n",
    "print(\"üìö Available documents in database:\\n\")\n",
    "\n",
    "doc_list = []\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(sql_text(\"\"\"\n",
    "        SELECT id, original_filename, filename, file_size, status \n",
    "        FROM document \n",
    "        ORDER BY uploaded_at DESC\n",
    "    \"\"\"))\n",
    "    \n",
    "    for i, row in enumerate(result.fetchall(), 1):\n",
    "        doc_id, original_filename, storage_filename, file_size, status = row\n",
    "        doc_list.append({\n",
    "            \"index\": i,\n",
    "            \"id\": doc_id,\n",
    "            \"storage_filename\": storage_filename,\n",
    "            \"original_filename\": original_filename,\n",
    "            \"file_size\": file_size,\n",
    "            \"status\": status\n",
    "        })\n",
    "        print(f\"  [{i}] {original_filename}\")\n",
    "        print(f\"      Status: {status} | Size: {file_size:,} bytes\")\n",
    "        print(f\"      ID: {doc_id}\\n\")\n",
    "\n",
    "if not doc_list:\n",
    "    print(\"  ‚ö†Ô∏è  No documents found. Upload documents via frontend first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT WHICH DOCUMENTS TO INGEST\n",
    "# Change this list to select which documents to process\n",
    "doc_indices = [1, 2]  # Process first 2 documents\n",
    "\n",
    "selected_documents = []\n",
    "if doc_list:\n",
    "    print(f\"üìã Selected documents to ingest:\\n\")\n",
    "    for idx in doc_indices:\n",
    "        doc = next((d for d in doc_list if d[\"index\"] == idx), None)\n",
    "        if doc:\n",
    "            selected_documents.append(doc)\n",
    "            print(f\"  ‚úÖ [{idx}] {doc['original_filename']}\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå [{idx}] Invalid index\")\n",
    "    \n",
    "    if selected_documents:\n",
    "        print(f\"\\n‚úÖ Total: {len(selected_documents)} document(s) to process\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå No valid documents selected\")\n",
    "else:\n",
    "    print(\"‚ùå No documents available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Extract Text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import io\n",
    "\n",
    "def extract_text_from_pdf(pdf_bytes: bytes) -> dict:\n",
    "    \"\"\"\n",
    "    Extract text from PDF with page tracking.\n",
    "    Returns: {page_number: text_content}\n",
    "    \"\"\"\n",
    "    pages_text = {}\n",
    "    try:\n",
    "        with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:\n",
    "            for page_num, page in enumerate(pdf.pages, 1):\n",
    "                extracted = page.extract_text()\n",
    "                if extracted and extracted.strip():\n",
    "                    pages_text[page_num] = extracted\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting PDF: {e}\")\n",
    "        raise\n",
    "    return pages_text\n",
    "\n",
    "print(\"‚úÖ PDF extraction function loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from selected documents\n",
    "extracted_texts = {}\n",
    "\n",
    "if selected_documents:\n",
    "    print(f\"üìÑ Extracting text from {len(selected_documents)} document(s)...\\n\")\n",
    "    \n",
    "    for doc in selected_documents:\n",
    "        doc_id = doc[\"id\"]\n",
    "        storage_filename = doc[\"storage_filename\"]\n",
    "        original_filename = doc[\"original_filename\"]\n",
    "        \n",
    "        try:\n",
    "            pdf_bytes = storage.get(storage_filename)\n",
    "            pages_text = extract_text_from_pdf(pdf_bytes)\n",
    "            extracted_texts[doc_id] = pages_text\n",
    "            \n",
    "            total_chars = sum(len(t) for t in pages_text.values())\n",
    "            print(f\"  ‚úÖ {original_filename}\")\n",
    "            print(f\"     Pages: {len(pages_text)}, Characters: {total_chars:,}\\n\")\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"  ‚ùå {original_filename}: File not found in storage\")\n",
    "            print(f\"     Looking for: storage/uploads/{storage_filename}.pdf\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå {original_filename}: {e}\\n\")\n",
    "    \n",
    "    if extracted_texts:\n",
    "        print(f\"‚úÖ Successfully extracted {len(extracted_texts)} document(s)\")\n",
    "    else:\n",
    "        print(f\"‚ùå No documents extracted successfully\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No documents selected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Chunk Text (with Page Numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def chunk_text_with_pages(\n",
    "    pages_text: Dict[int, str], \n",
    "    chunk_size: int = 500, \n",
    "    overlap: int = 50\n",
    ") -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Chunk text while tracking page numbers.\n",
    "    Returns: [(chunk_content, page_number), ...]\n",
    "    \"\"\"\n",
    "    chunks_with_pages = []\n",
    "    \n",
    "    for page_num in sorted(pages_text.keys()):\n",
    "        page_content = pages_text[page_num]\n",
    "        # Split by sentence boundaries\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', page_content)\n",
    "        \n",
    "        current_chunk = []\n",
    "        current_size = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            words = sentence.split()\n",
    "            if not words:\n",
    "                continue\n",
    "            \n",
    "            # If adding words would exceed chunk_size, save current chunk\n",
    "            if current_size + len(words) > chunk_size and current_chunk:\n",
    "                chunk_content = ' '.join(current_chunk)\n",
    "                chunks_with_pages.append((chunk_content, page_num))\n",
    "                # Overlap: keep last ~10% of words\n",
    "                current_chunk = current_chunk[-max(1, int(len(current_chunk) * 0.1)):]\n",
    "                current_size = len(' '.join(current_chunk).split())\n",
    "            \n",
    "            current_chunk.extend(words)\n",
    "            current_size += len(words)\n",
    "        \n",
    "        # Add final chunk for this page\n",
    "        if current_chunk:\n",
    "            chunk_content = ' '.join(current_chunk)\n",
    "            chunks_with_pages.append((chunk_content, page_num))\n",
    "    \n",
    "    return chunks_with_pages\n",
    "\n",
    "print(\"‚úÖ Text chunking function loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chunks for all extracted documents\n",
    "chunks_by_document = {}\n",
    "\n",
    "if extracted_texts:\n",
    "    print(f\"üì¶ Creating chunks for {len(extracted_texts)} document(s)...\\n\")\n",
    "    \n",
    "    for doc_id, pages_text in extracted_texts.items():\n",
    "        chunks_with_pages = chunk_text_with_pages(pages_text, chunk_size=500, overlap=50)\n",
    "        chunks_by_document[doc_id] = chunks_with_pages\n",
    "        \n",
    "        # Get filename\n",
    "        doc = next(d for d in selected_documents if d[\"id\"] == doc_id)\n",
    "        filename = doc[\"original_filename\"]\n",
    "        \n",
    "        print(f\"  ‚úÖ {filename}\")\n",
    "        print(f\"     Chunks: {len(chunks_with_pages)}\")\n",
    "        if chunks_with_pages:\n",
    "            content, page_num = chunks_with_pages[0]\n",
    "            print(f\"     Sample: Page {page_num}: {content[:80]}...\\n\")\n",
    "    \n",
    "    total_chunks = sum(len(c) for c in chunks_by_document.values())\n",
    "    print(f\"‚úÖ Total chunks created: {total_chunks}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No extracted text to chunk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Generate Embeddings with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from typing import List\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY not set in .env\")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def generate_embedding(text: str) -> List[float]:\n",
    "    \"\"\"Generate 1536-dimensional embedding using text-embedding-3-small\"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "print(\"‚úÖ OpenAI embedding function loaded (1536 dimensions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test embedding on first chunk\n",
    "if chunks_by_document:\n",
    "    print(\"üß™ Testing embedding generation...\\n\")\n",
    "    \n",
    "    first_doc_id = list(chunks_by_document.keys())[0]\n",
    "    first_chunks = chunks_by_document[first_doc_id]\n",
    "    \n",
    "    if first_chunks:\n",
    "        chunk_content, page_num = first_chunks[0]\n",
    "        doc = next(d for d in selected_documents if d[\"id\"] == first_doc_id)\n",
    "        \n",
    "        print(f\"  Document: {doc['original_filename']}\")\n",
    "        print(f\"  Page: {page_num}\")\n",
    "        print(f\"  Chunk text: {chunk_content[:100]}...\\n\")\n",
    "        \n",
    "        embedding = generate_embedding(chunk_content)\n",
    "        print(f\"  ‚úÖ Embedding generated\")\n",
    "        print(f\"     Dimensions: {len(embedding)}\")\n",
    "        print(f\"     First 5 values: {embedding[:5]}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No chunks to test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Insert Chunks into Database with Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def insert_chunks_batch(doc_id: str, chunks_with_pages: List[Tuple[str, int]], batch_size: int = 10):\n",
    "    \"\"\"\n",
    "    Insert chunks with embeddings into database.\n",
    "    Embeddings stored as JSON array (TEXT) for LOCAL development.\n",
    "    \"\"\"\n",
    "    db = SessionLocal()\n",
    "    \n",
    "    try:\n",
    "        # Update document status to PROCESSING\n",
    "        db.execute(sql_text(\"\"\"\n",
    "            UPDATE document SET status = 'PROCESSING' WHERE id = :id\n",
    "        \"\"\"), {\"id\": doc_id})\n",
    "        db.commit()\n",
    "        \n",
    "        # Get filename for logging\n",
    "        result = db.execute(sql_text(\n",
    "            \"SELECT original_filename FROM document WHERE id = :id\"\n",
    "        ), {\"id\": doc_id})\n",
    "        filename = result.scalar()\n",
    "        \n",
    "        # Insert chunks in batches\n",
    "        print(f\"  Inserting {len(chunks_with_pages)} chunks into {filename}...\")\n",
    "        \n",
    "        for idx, (chunk_content, page_number) in enumerate(chunks_with_pages):\n",
    "            # Generate embedding\n",
    "            embedding = generate_embedding(chunk_content)\n",
    "            # Store as JSON array string\n",
    "            embedding_json = json.dumps(embedding)\n",
    "            \n",
    "            # Chunk metadata\n",
    "            chunk_metadata = {\n",
    "                \"page\": page_number,\n",
    "                \"chunk_index\": idx\n",
    "            }\n",
    "            \n",
    "            # Insert into database\n",
    "            db.execute(sql_text(\"\"\"\n",
    "                INSERT INTO document_chunk \n",
    "                (id, document_id, chunk_index, content, page_number, embedding, chunk_metadata, created_at)\n",
    "                VALUES \n",
    "                (gen_random_uuid(), :doc_id, :chunk_idx, :content, :page_num, :embedding, :metadata, now())\n",
    "            \"\"\"), {\n",
    "                \"doc_id\": doc_id,\n",
    "                \"chunk_idx\": idx,\n",
    "                \"content\": chunk_content,\n",
    "                \"page_num\": page_number,\n",
    "                \"embedding\": embedding_json,\n",
    "                \"metadata\": json.dumps(chunk_metadata)\n",
    "            })\n",
    "            \n",
    "            # Commit batch every N chunks\n",
    "            if (idx + 1) % batch_size == 0:\n",
    "                db.commit()\n",
    "                print(f\"     ‚úì {idx + 1}/{len(chunks_with_pages)}\")\n",
    "        \n",
    "        # Commit remaining\n",
    "        db.commit()\n",
    "        print(f\"     ‚úì {len(chunks_with_pages)}/{len(chunks_with_pages)} complete\")\n",
    "        \n",
    "        # Update document status to PROCESSED\n",
    "        db.execute(sql_text(\"\"\"\n",
    "            UPDATE document \n",
    "            SET status = 'PROCESSED', processed_at = :now \n",
    "            WHERE id = :id\n",
    "        \"\"\"), {\"now\": datetime.utcnow(), \"id\": doc_id})\n",
    "        db.commit()\n",
    "        print(f\"  ‚úÖ Document marked as PROCESSED\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        db.execute(sql_text(\"\"\"\n",
    "            UPDATE document SET status = 'FAILED' WHERE id = :id\n",
    "        \"\"\"), {\"id\": doc_id})\n",
    "        db.commit()\n",
    "        print(f\"  ‚ùå Error: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "print(\"‚úÖ Batch insert function loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## üöÄ EXECUTE - Ingest All Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è CLEAN UP OLD DATA FIRST\n",
    "print(\"üßπ Cleaning up old embeddings...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    # Get IDs of documents we're about to ingest\n",
    "    doc_ids_to_clean = [d[\"id\"] for d in selected_documents]\n",
    "    \n",
    "    # Delete chunks for these documents only\n",
    "    for doc_id in doc_ids_to_clean:\n",
    "        conn.execute(sql_text(\"\"\"\n",
    "            DELETE FROM document_chunk WHERE document_id = :doc_id\n",
    "        \"\"\"), {\"doc_id\": doc_id})\n",
    "    \n",
    "    # Reset documents to UPLOADED status\n",
    "    for doc_id in doc_ids_to_clean:\n",
    "        conn.execute(sql_text(\"\"\"\n",
    "            UPDATE document \n",
    "            SET status = 'UPLOADED', processed_at = NULL \n",
    "            WHERE id = :doc_id\n",
    "        \"\"\"), {\"doc_id\": doc_id})\n",
    "    \n",
    "    conn.commit()\n",
    "\n",
    "print(f\"‚úÖ Cleaned {len(selected_documents)} document(s)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ START INGESTION\n",
    "if chunks_by_document:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üöÄ STARTING INGESTION PIPELINE\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "    \n",
    "    for doc_id, chunks in chunks_by_document.items():\n",
    "        doc = next(d for d in selected_documents if d[\"id\"] == doc_id)\n",
    "        print(f\"üìÑ Processing: {doc['original_filename']}\")\n",
    "        insert_chunks_batch(doc_id, chunks, batch_size=10)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚úÖ INGESTION COMPLETE!\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"‚ùå No chunks to ingest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Verify Ingestion - Check Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÖ VERIFICATION - Database Contents\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    # Check total chunks\n",
    "    result = conn.execute(sql_text(\"\"\"\n",
    "        SELECT COUNT(*) as total, COUNT(DISTINCT document_id) as documents\n",
    "        FROM document_chunk\n",
    "    \"\"\")).fetchone()\n",
    "    \n",
    "    total_chunks, num_docs = result\n",
    "    print(f\"Total chunks: {total_chunks:,}\")\n",
    "    print(f\"Documents processed: {num_docs}\\n\")\n",
    "    \n",
    "    # Check document statuses\n",
    "    result = conn.execute(sql_text(\"\"\"\n",
    "        SELECT d.original_filename, d.status, COUNT(dc.id) as chunk_count\n",
    "        FROM document d\n",
    "        LEFT JOIN document_chunk dc ON d.id = dc.document_id\n",
    "        GROUP BY d.id, d.original_filename, d.status\n",
    "        ORDER BY d.uploaded_at DESC\n",
    "    \"\"\")).fetchall()\n",
    "    \n",
    "    print(\"Document Status:\")\n",
    "    for filename, status, chunk_count in result:\n",
    "        print(f\"  ‚Ä¢ {filename}\")\n",
    "        print(f\"    Status: {status}, Chunks: {chunk_count:,}\\n\")\n",
    "    \n",
    "    # Sample embedding\n",
    "    result = conn.execute(sql_text(\"\"\"\n",
    "        SELECT embedding FROM document_chunk LIMIT 1\n",
    "    \"\"\")).scalar()\n",
    "    \n",
    "    if result:\n",
    "        embedding_data = json.loads(result)\n",
    "        print(f\"Sample Embedding:\")\n",
    "        print(f\"  Dimensions: {len(embedding_data)}\")\n",
    "        print(f\"  First 5 values: {embedding_data[:5]}\")\n",
    "        if len(embedding_data) == 1536:\n",
    "            print(f\"  ‚úÖ Correct: 1536 dimensions!\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå Wrong: expected 1536 dimensions\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Test Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search_local(query_text: str, top_k: int = 5) -> list:\n",
    "    \"\"\"\n",
    "    Semantic search using embedding similarity.\n",
    "    For LOCAL: manual cosine similarity since pgvector unavailable\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    db = SessionLocal()\n",
    "    try:\n",
    "        # Generate query embedding\n",
    "        query_embedding = np.array(generate_embedding(query_text))\n",
    "        \n",
    "        # Get all chunks with embeddings\n",
    "        result = db.execute(sql_text(\"\"\"\n",
    "            SELECT \n",
    "                dc.content,\n",
    "                d.original_filename,\n",
    "                dc.page_number,\n",
    "                dc.embedding\n",
    "            FROM document_chunk dc\n",
    "            JOIN document d ON dc.document_id = d.id\n",
    "            WHERE d.status = 'PROCESSED'\n",
    "            ORDER BY dc.created_at DESC\n",
    "            LIMIT 100\n",
    "        \"\"\")).fetchall()\n",
    "        \n",
    "        # Calculate similarity for each chunk\n",
    "        similarities = []\n",
    "        for content, filename, page_num, embedding_json in result:\n",
    "            chunk_embedding = np.array(json.loads(embedding_json))\n",
    "            # Cosine similarity = dot product / (norm1 * norm2)\n",
    "            similarity = np.dot(query_embedding, chunk_embedding) / (\n",
    "                np.linalg.norm(query_embedding) * np.linalg.norm(chunk_embedding)\n",
    "            )\n",
    "            similarities.append((content, filename, page_num, float(similarity)))\n",
    "        \n",
    "        # Sort by similarity and return top_k\n",
    "        similarities.sort(key=lambda x: x[3], reverse=True)\n",
    "        return [\n",
    "            {\n",
    "                \"content\": content,\n",
    "                \"filename\": filename,\n",
    "                \"page\": page_num,\n",
    "                \"similarity\": similarity\n",
    "            }\n",
    "            for content, filename, page_num, similarity in similarities[:top_k]\n",
    "        ]\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "print(\"‚úÖ Semantic search function loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test semantic search\n",
    "print(\"üéØ Testing Semantic Search\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "QUERY = \"software engineering\"  # Change this to test different queries\n",
    "\n",
    "print(f\"Query: \\\"{QUERY}\\\"\\n\")\n",
    "\n",
    "results = semantic_search_local(QUERY, top_k=5)\n",
    "\n",
    "if results:\n",
    "    print(f\"Found {len(results)} relevant chunks:\\n\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"  [{i}] üìÑ {result['filename']}\")\n",
    "        print(f\"      üìç Page {result['page']}\")\n",
    "        print(f\"      ‚≠ê Similarity: {result['similarity']:.4f}\")\n",
    "        print(f\"      üìù {result['content'][:150]}...\\n\")\n",
    "else:\n",
    "    print(\"‚ùå No results found\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
