{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ RAG Document Ingestion Pipeline\n",
    "\n",
    "Step-by-step pipeline: PDF â†’ Text â†’ Chunks â†’ Embeddings â†’ pgvector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Setup Database Connection & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Database engine created\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text as sql_text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "load_dotenv(Path(\".env\"))\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "DATABASE_URL = os.getenv(\"DATABASE_URL\")\n",
    "if \"localhost:5432\" in DATABASE_URL:\n",
    "    DATABASE_URL = DATABASE_URL.replace(\"localhost:5432\", \"127.0.0.1:5433\")\n",
    "\n",
    "engine = create_engine(DATABASE_URL, pool_pre_ping=True, pool_size=5, pool_recycle=3600)\n",
    "SessionLocal = sessionmaker(bind=engine)\n",
    "\n",
    "print(\"âœ… Database engine created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Connected to: system_llm\n",
      "âœ… PostgreSQL: PostgreSQL 15.14 on x86_64-pc-linux-gnu\n"
     ]
    }
   ],
   "source": [
    "with engine.connect() as conn:\n",
    "    result = conn.execute(sql_text(\"SELECT current_database(), version()\"))\n",
    "    db_name, version = result.fetchone()\n",
    "    print(f\"âœ… Connected to: {db_name}\")\n",
    "    print(f\"âœ… PostgreSQL: {version.split(',')[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Setup Storage Provider & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Storage provider: GCSStorageProvider\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "class LocalFileStorage:\n",
    "    def __init__(self, base_path=\"storage/uploads\"):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.base_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def get(self, file_id: str) -> bytes:\n",
    "        path = self.base_path / f\"{file_id}.pdf\"\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"File not found: {file_id}\")\n",
    "        return path.read_bytes()\n",
    "\n",
    "class GCSStorageProvider:\n",
    "    def __init__(self, bucket_name: str, credentials_path: str = None, project_id: str = None):\n",
    "        from google.cloud import storage as gcs_storage\n",
    "        from google.oauth2 import service_account\n",
    "        \n",
    "        if bucket_name.startswith('gs://'):\n",
    "            bucket_name = bucket_name[5:]\n",
    "        \n",
    "        self.bucket_name = bucket_name\n",
    "        \n",
    "        if credentials_path and os.path.exists(credentials_path):\n",
    "            creds = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "            self.client = gcs_storage.Client(credentials=creds, project=project_id or creds.project_id)\n",
    "        else:\n",
    "            self.client = gcs_storage.Client(project=project_id)\n",
    "        \n",
    "        self.bucket = self.client.bucket(self.bucket_name)\n",
    "    \n",
    "    def get(self, file_id: str) -> bytes:\n",
    "        blob = self.bucket.blob(f\"uploads/{file_id}.pdf\")\n",
    "        if not blob.exists():\n",
    "            raise FileNotFoundError(f\"File not found: {file_id}\")\n",
    "        return blob.download_as_bytes()\n",
    "\n",
    "STORAGE_TYPE = os.getenv(\"STORAGE_TYPE\", \"local\").lower()\n",
    "if STORAGE_TYPE == \"gcs\":\n",
    "    storage = GCSStorageProvider(\n",
    "        os.getenv(\"GCS_BUCKET_NAME\"),\n",
    "        os.getenv(\"GCS_CREDENTIALS_PATH\"),\n",
    "        os.getenv(\"GCS_PROJECT_ID\")\n",
    "    )\n",
    "else:\n",
    "    storage = LocalFileStorage()\n",
    "\n",
    "print(f\"âœ… Storage provider: {storage.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GCS storage configured\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "test_id = str(uuid.uuid4())\n",
    "test_content = b\"Test storage\"\n",
    "\n",
    "if isinstance(storage, LocalFileStorage):\n",
    "    path = storage.base_path / f\"{test_id}.pdf\"\n",
    "    path.write_bytes(test_content)\n",
    "    retrieved = storage.get(test_id)\n",
    "    assert retrieved == test_content\n",
    "    path.unlink()\n",
    "    print(f\"âœ… Storage test passed\")\n",
    "else:\n",
    "    print(f\"âœ… GCS storage configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Checking storage contents...\n",
      "\n",
      "Bucket: system-llm-storage\n",
      "\n",
      "Files in bucket:\n",
      "  1. uploads/842563bf-8647-42f8-8704-49690fee2b95.pdf\n",
      "  2. uploads/af5020f2-593d-4e2f-8560-f36bcea3236a.pdf\n",
      "\n",
      "Total files: 2\n"
     ]
    }
   ],
   "source": [
    "## ğŸ” Diagnostic: Check Storage Contents\n",
    "\n",
    "print(\"ğŸ” Checking storage contents...\\n\")\n",
    "\n",
    "if isinstance(storage, GCSStorageProvider):\n",
    "    print(f\"Bucket: {storage.bucket_name}\\n\")\n",
    "    print(\"Files in bucket:\")\n",
    "    \n",
    "    blobs = storage.client.list_blobs(storage.bucket_name, prefix=\"uploads/\")\n",
    "    blob_list = list(blobs)\n",
    "    \n",
    "    if blob_list:\n",
    "        for i, blob in enumerate(blob_list[:20], 1):\n",
    "            print(f\"  {i}. {blob.name}\")\n",
    "        if len(blob_list) > 20:\n",
    "            print(f\"  ... and {len(blob_list) - 20} more\")\n",
    "    else:\n",
    "        print(\"  (empty - no files uploaded yet)\")\n",
    "    \n",
    "    print(f\"\\nTotal files: {len(blob_list)}\")\n",
    "else:\n",
    "    print(\"Using LocalFileStorage\")\n",
    "    storage_files = list(storage.base_path.glob(\"*.pdf\"))\n",
    "    if storage_files:\n",
    "        for f in storage_files[:10]:\n",
    "            print(f\"  {f.name}\")\n",
    "    else:\n",
    "        print(\"  (no PDF files found)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Retrieve Documents for Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Documents available for RAG processing:\n",
      "\n",
      "  [1] Software-Engineering-9th-Edition-by-Ian-Sommerville.pdf\n",
      "      Size: 5397464 bytes\n",
      "\n",
      "  [2] Software Engineering - Roger S Pressman [5th edition].pdf\n",
      "      Size: 6986228 bytes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“š Documents available for RAG processing:\\n\")\n",
    "\n",
    "\n",
    "doc_list = []  # Store dicts with doc metadata\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(sql_text(\"\"\"\n",
    "        SELECT id, original_filename, filename, file_size, status \n",
    "        FROM document \n",
    "        LIMIT 10\n",
    "    \"\"\"))\n",
    "    \n",
    "    docs = result.fetchall()\n",
    "    if docs:\n",
    "        for i, row in enumerate(docs, 1):\n",
    "            doc_id, original_filename, storage_filename, file_size, status = row\n",
    "            # Store as dict for clarity\n",
    "            doc_list.append({\n",
    "                \"index\": i,\n",
    "                \"db_id\": doc_id,\n",
    "                \"storage_filename\": storage_filename,\n",
    "                \"original_filename\": original_filename,\n",
    "                \"file_size\": file_size\n",
    "            })\n",
    "            print(f\"  [{i}] {original_filename}\")\n",
    "            print(f\"      Size: {file_size} bytes\\n\")\n",
    "    else:\n",
    "        print(\"  âš ï¸  No documents to process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Selected documents:\n",
      "\n",
      "  âœ… [1] Software-Engineering-9th-Edition-by-Ian-Sommerville.pdf (5397464 bytes)\n",
      "  âœ… [2] Software Engineering - Roger S Pressman [5th edition].pdf (6986228 bytes)\n",
      "\n",
      "âœ… Total documents to process: 2\n"
     ]
    }
   ],
   "source": [
    "# Select documents to process (as list of indices)\n",
    "# Example: [1, 2, 3] to process all, or [1, 3] to process first and third\n",
    "\n",
    "doc_indices = [1, 2]  # Change this list\n",
    "\n",
    "selected_documents = []  # Store selected document dicts\n",
    "\n",
    "if doc_list:\n",
    "    print(f\"ğŸ“‹ Selected documents:\\n\")\n",
    "    for idx in doc_indices:\n",
    "        # Find document with matching index\n",
    "        doc = next((d for d in doc_list if d[\"index\"] == idx), None)\n",
    "        if doc:\n",
    "            selected_documents.append(doc)\n",
    "            print(f\"  âœ… [{idx}] {doc['original_filename']} ({doc['file_size']} bytes)\")\n",
    "        else:\n",
    "            print(f\"  âŒ [{idx}] Invalid index (out of range)\")\n",
    "\n",
    "    if not selected_documents:\n",
    "        print(\"âŒ No valid documents selected\")\n",
    "    else:\n",
    "        print(f\"\\nâœ… Total documents to process: {len(selected_documents)}\")\n",
    "else:\n",
    "    print(\"âŒ No documents available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Text Extraction from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PDF extraction function loaded (with page tracking)\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import io\n",
    "\n",
    "def extract_text_from_pdf(pdf_bytes: bytes) -> dict:\n",
    "    \"\"\"\n",
    "    Extract text from PDF with page number tracking.\n",
    "    Returns: {page_number: [full_text_for_page, ...]}\n",
    "    \"\"\"\n",
    "    pages_text = {}\n",
    "    try:\n",
    "        with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:\n",
    "            for page_num, page in enumerate(pdf.pages, 1):  # Start from page 1\n",
    "                extracted = page.extract_text()\n",
    "                if extracted:\n",
    "                    pages_text[page_num] = extracted\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting PDF: {e}\")\n",
    "        raise\n",
    "    return pages_text\n",
    "\n",
    "print(\"âœ… PDF extraction function loaded (with page tracking)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Extracting text from 2 documents...\n",
      "\n",
      "  âœ… Software-Engineering-9th-Edition-by-Ian-Sommerville.pdf: 789 pages, 1968108 characters\n",
      "  âœ… Software Engineering - Roger S Pressman [5th edition].pdf: 867 pages, 2192245 characters\n",
      "\n",
      "âœ… Extracted 2 documents with page tracking\n"
     ]
    }
   ],
   "source": [
    "extracted_texts = {}  # Dictionary to store text with page numbers: {db_id: {page_num: text}}\n",
    "\n",
    "if selected_documents:\n",
    "    print(f\"ğŸ“„ Extracting text from {len(selected_documents)} documents...\\n\")\n",
    "    \n",
    "    for doc in selected_documents:\n",
    "        db_id = doc[\"db_id\"]\n",
    "        storage_filename = doc[\"storage_filename\"]\n",
    "        filename = doc[\"original_filename\"]\n",
    "        \n",
    "        try:\n",
    "            # Use storage_filename to retrieve from storage!\n",
    "            pdf_bytes = storage.get(storage_filename)\n",
    "            pages_text = extract_text_from_pdf(pdf_bytes)  # Returns {page_num: text}\n",
    "            extracted_texts[db_id] = pages_text\n",
    "            \n",
    "            total_chars = sum(len(t) for t in pages_text.values())\n",
    "            print(f\"  âœ… {filename}: {len(pages_text)} pages, {total_chars} characters\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ {filename}: {e}\")\n",
    "    \n",
    "    if extracted_texts:\n",
    "        print(f\"\\nâœ… Extracted {len(extracted_texts)} documents with page tracking\")\n",
    "    else:\n",
    "        print(f\"âŒ No documents extracted successfully\")\n",
    "else:\n",
    "    print(\"âš ï¸  No documents selected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Execute Text Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Text chunking function loaded (with page tracking)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def chunk_text_with_pages(pages_text: Dict[int, str], chunk_size: int = 500, overlap: int = 50) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Chunk text while preserving page numbers.\n",
    "    Returns: [(chunk_content, page_number), ...]\n",
    "    \"\"\"\n",
    "    chunks_with_pages = []\n",
    "    \n",
    "    for page_num in sorted(pages_text.keys()):\n",
    "        page_content = pages_text[page_num]  # Use 'page_content' instead of 'text' to avoid shadowing\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', page_content)\n",
    "        \n",
    "        current_chunk = []\n",
    "        current_size = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            words = sentence.split()\n",
    "            if current_size + len(words) > chunk_size and current_chunk:\n",
    "                chunk_content = ' '.join(current_chunk)\n",
    "                chunks_with_pages.append((chunk_content, page_num))\n",
    "                current_chunk = current_chunk[-int(overlap/10):]\n",
    "                current_size = len(' '.join(current_chunk).split())\n",
    "            \n",
    "            current_chunk.extend(words)\n",
    "            current_size += len(words)\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunk_content = ' '.join(current_chunk)\n",
    "            chunks_with_pages.append((chunk_content, page_num))\n",
    "    \n",
    "    return chunks_with_pages\n",
    "\n",
    "print(\"âœ… Text chunking function loaded (with page tracking)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Creating chunks for 2 documents...\n",
      "\n",
      "  âœ… Software-Engineering-9th-Edition-by-Ian-Sommerville.pdf: 866 chunks\n",
      "     Sample: Page 2: SOFTWARE ENGINEERING Ninth Edition Ian Sommerville Addison-Wesley Boston Columbu...\n",
      "  âœ… Software Engineering - Roger S Pressman [5th edition].pdf: 959 chunks\n",
      "     Sample: Page 1: Software Engineering A P R A C T I T I O N E R â€™ S A P P R O A C H...\n",
      "\n",
      "âœ… Total chunks created: 1825\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import text as sql_text\n",
    "\n",
    "chunks_by_document = {}  # Dictionary to store chunks for each document: {db_id: [(content, page_num), ...]}\n",
    "\n",
    "if extracted_texts:\n",
    "    print(f\"ğŸ“¦ Creating chunks for {len(extracted_texts)} documents...\\n\")\n",
    "    \n",
    "    for doc_id, pages_text in extracted_texts.items():\n",
    "        chunks_with_pages = chunk_text_with_pages(pages_text, chunk_size=500)\n",
    "        chunks_by_document[doc_id] = chunks_with_pages\n",
    "        \n",
    "        # Get filename for display\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(sql_text(\n",
    "                \"SELECT original_filename FROM document WHERE id = :id\"\n",
    "            ), {\"id\": doc_id})\n",
    "            filename = result.scalar()\n",
    "        \n",
    "        print(f\"  âœ… {filename}: {len(chunks_with_pages)} chunks\")\n",
    "        if chunks_with_pages:\n",
    "            content, page_num = chunks_with_pages[0]\n",
    "            print(f\"     Sample: Page {page_num}: {content[:80]}...\")\n",
    "    \n",
    "    total_chunks = sum(len(c) for c in chunks_by_document.values())\n",
    "    print(f\"\\nâœ… Total chunks created: {total_chunks}\")\n",
    "else:\n",
    "    print(\"âš ï¸  No extracted text to chunk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Load OpenAI & Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OpenAI embedding function loaded (1536 dimensions)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY not set in .env\")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def generate_embedding(text: str) -> List[float]:\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    # Use full 1536 dimensions (updated to match database schema)\n",
    "    embedding = response.data[0].embedding\n",
    "    return embedding\n",
    "\n",
    "print(\"âœ… OpenAI embedding function loaded (1536 dimensions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— Generating sample embedding...\n",
      "\n",
      "  Testing: Software-Engineering-9th-Edition-by-Ian-Sommerville.pdf (Page 2)\n",
      "  âœ… Embedding generated\n",
      "     Dimension: 1536\n",
      "     First 5 values: [0.010699484497308731, 0.03421851247549057, 0.004125438630580902, -0.016799306496977806, 0.0024005649611353874]\n"
     ]
    }
   ],
   "source": [
    "if chunks_by_document:\n",
    "    print(\"ğŸ”— Generating sample embedding...\\n\")\n",
    "    \n",
    "    for doc_id, chunks_with_pages in list(chunks_by_document.items())[:1]:  # Just test first document\n",
    "        if chunks_with_pages:\n",
    "            # Get filename for display\n",
    "            with engine.connect() as conn:\n",
    "                result = conn.execute(sql_text(\n",
    "                    \"SELECT original_filename FROM document WHERE id = :id\"\n",
    "                ), {\"id\": doc_id})\n",
    "                filename = result.scalar()\n",
    "            \n",
    "            chunk_content, page_num = chunks_with_pages[0]\n",
    "            print(f\"  Testing: {filename} (Page {page_num})\")\n",
    "            embedding = generate_embedding(chunk_content)\n",
    "            \n",
    "            print(f\"  âœ… Embedding generated\")\n",
    "            print(f\"     Dimension: {len(embedding)}\")\n",
    "            print(f\"     First 5 values: {embedding[:5]}\")\n",
    "else:\n",
    "    print(\"âš ï¸  No chunks to embed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] [Config] Settings initialized\n",
      "[DEBUG] [Config] SECRET_KEY length: 64\n",
      "[DEBUG] [Config] SECRET_KEY: 9f93b17cefac9425415a3829d8ccd5f7db8bc2dcea81f2321236a9aa0158b268\n",
      "[DEBUG] [Config] SECRET_KEY hex: 39663933623137636566616339343235343135613338323964386363643566376462386263326463656138316632333231323336613961613031353862323638\n",
      "[DEBUG] [Config] DATABASE_URL: postgresql://llm_user:anLLMUser123123@127.0.0.1:54...\n",
      "[DEBUG] [Config] ALGORITHM: HS256\n",
      "âœ… Insert function FIXED - using SQLAlchemy ORM with pgvector support\n"
     ]
    }
   ],
   "source": [
    "# from datetime import datetime\n",
    "# import json\n",
    "# from app.models.document_chunk import DocumentChunk\n",
    "\n",
    "# def insert_chunks_and_finalize(document_id: str, chunks_with_pages: list):\n",
    "#     \"\"\"\n",
    "#     Insert chunks ke PostgreSQL dengan pgvector embeddings (1536 dimensions)\n",
    "#     FIXED: Use SQLAlchemy ORM untuk handle vector type dengan benar\n",
    "#     \"\"\"\n",
    "#     db = SessionLocal()\n",
    "#     try:\n",
    "#         # 1. Update status to PROCESSING\n",
    "#         db.execute(sql_text(\"\"\"\n",
    "#             UPDATE document SET status = 'PROCESSING' WHERE id = :id\n",
    "#         \"\"\"), {\"id\": document_id})\n",
    "#         db.commit()\n",
    "#         print(f\"  âœ“ Status: PROCESSING\")\n",
    "        \n",
    "#         # 2. Generate embeddings and insert chunks\n",
    "#         print(f\"  âœ“ Inserting {len(chunks_with_pages)} chunks...\")\n",
    "#         for idx, (chunk_content, page_number) in enumerate(chunks_with_pages):\n",
    "#             embedding = generate_embedding(chunk_content)  # Returns List[float]\n",
    "            \n",
    "#             # Metadata JSON\n",
    "#             chunk_metadata = {\n",
    "#                 \"page\": page_number,\n",
    "#                 \"chunk_sequence\": idx\n",
    "#             }\n",
    "            \n",
    "#             # âœ… FIXED: Use ORM object directly\n",
    "#             chunk = DocumentChunk(\n",
    "#                 document_id=document_id,\n",
    "#                 chunk_index=idx,\n",
    "#                 content=chunk_content,\n",
    "#                 page_number=page_number,\n",
    "#                 embedding=embedding,  # Pass as list - SQLAlchemy handles pgvector conversion\n",
    "#                 chunk_metadata=chunk_metadata\n",
    "#             )\n",
    "            \n",
    "#             db.add(chunk)\n",
    "#             db.flush()  # Flush to ensure it's saved but don't commit yet\n",
    "            \n",
    "#             if (idx + 1) % 5 == 0:\n",
    "#                 print(f\"     {idx + 1}/{len(chunks_with_pages)} inserted...\")\n",
    "        \n",
    "#         db.commit()  # Commit all at once\n",
    "#         print(f\"  âœ“ All {len(chunks_with_pages)} chunks inserted\")\n",
    "        \n",
    "#         # 3. Update document status to PROCESSED\n",
    "#         db.execute(sql_text(\"\"\"\n",
    "#             UPDATE document SET status = 'PROCESSED', processed_at = :now WHERE id = :id\n",
    "#         \"\"\"), {\"now\": datetime.utcnow(), \"id\": document_id})\n",
    "#         db.commit()\n",
    "#         print(f\"  âœ“ Status: PROCESSED\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         db.execute(sql_text(\"\"\"\n",
    "#             UPDATE document SET status = 'FAILED' WHERE id = :id\n",
    "#         \"\"\"), {\"id\": document_id})\n",
    "#         db.commit()\n",
    "#         print(f\"\\nâŒ Error: {e}\")\n",
    "#         raise\n",
    "#     finally:\n",
    "#         db.close()\n",
    "\n",
    "# print(\"âœ… Insert function FIXED - using SQLAlchemy ORM with pgvector support\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # âœ… STEP 1: Clean up old data (format string) from database\n",
    "\n",
    "# print(\"ğŸ§¹ Cleaning up old embeddings (format string)...\\n\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# with engine.connect() as conn:\n",
    "#     # DELETE all chunks - no matter what\n",
    "#     print(\"Step 1: Delete all document_chunk records...\")\n",
    "#     conn.execute(sql_text(\"TRUNCATE TABLE document_chunk CASCADE\"))\n",
    "#     conn.commit()\n",
    "#     print(\"âœ… TRUNCATED document_chunk table\")\n",
    "    \n",
    "#     # Reset ALL documents\n",
    "#     print(\"\\nStep 2: Reset ALL documents to UPLOADED status...\")\n",
    "#     conn.execute(sql_text(\"\"\"\n",
    "#         UPDATE document \n",
    "#         SET status = 'UPLOADED', processed_at = NULL\n",
    "#     \"\"\"))\n",
    "#     conn.commit()\n",
    "#     print(\"âœ… Reset all documents\")\n",
    "    \n",
    "#     # Verify\n",
    "#     chunk_count = conn.execute(sql_text(\"SELECT COUNT(*) FROM document_chunk\")).scalar()\n",
    "#     doc_count = conn.execute(sql_text(\"SELECT COUNT(*) FROM document WHERE status = 'UPLOADED'\")).scalar()\n",
    "    \n",
    "#     print(f\"\\nâœ… Chunks remaining: {chunk_count}\")\n",
    "#     print(f\"âœ… Documents ready for re-ingest: {doc_count}\")\n",
    "    \n",
    "#     print(\"\\n\" + \"=\" * 80)\n",
    "#     print(\"âœ… DATABASE CLEANED!\")\n",
    "#     print(\"\\nğŸš€ NEXT: Run '8ï¸âƒ£ EXECUTION - Process & Ingest All Documents'\")\n",
    "#     print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Quick Embedding Verification\n",
      "\n",
      "================================================================================\n",
      "Total chunks: 42\n",
      "Sample embedding raw: [-0.0046223043,0.026051002,0.028564312,-0.00636523...\n",
      "Sample embedding type (parsed): list\n",
      "Sample embedding dimensions: 1536\n",
      "âœ… CORRECT: 1536 dimensions!\n",
      "âœ… First 5 values: [-0.0046223043, 0.026051002, 0.028564312, -0.00636523, -0.04239844]\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# âœ… QUICK VERIFICATION - After re-ingestion\n",
    "\n",
    "print(\"âœ… Quick Embedding Verification\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def parse_vector_string(vector_str: str) -> list:\n",
    "    \"\"\"Parse PostgreSQL vector string to Python list\"\"\"\n",
    "    if not vector_str:\n",
    "        return []\n",
    "    # Remove brackets and split by comma\n",
    "    vector_str = vector_str.strip().strip('[]')\n",
    "    return [float(x.strip()) for x in vector_str.split(',') if x.strip()]\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    # Get sample embedding\n",
    "    result = conn.execute(sql_text(\"\"\"\n",
    "        SELECT COUNT(*) as total, \n",
    "               (SELECT embedding FROM document_chunk LIMIT 1) as sample_emb\n",
    "        FROM document_chunk\n",
    "    \"\"\")).fetchone()\n",
    "    \n",
    "    total, sample = result\n",
    "    \n",
    "    print(f\"Total chunks: {total}\")\n",
    "    \n",
    "    if sample:\n",
    "        # Parse vector string to list\n",
    "        if isinstance(sample, str):\n",
    "            sample_array = parse_vector_string(sample)\n",
    "        else:\n",
    "            sample_array = sample\n",
    "        \n",
    "        print(f\"Sample embedding raw: {sample[:50]}...\" if len(sample) > 50 else f\"Sample embedding raw: {sample}\")\n",
    "        print(f\"Sample embedding type (parsed): {type(sample_array).__name__}\")\n",
    "        print(f\"Sample embedding dimensions: {len(sample_array)}\")\n",
    "        \n",
    "        if len(sample_array) == 1536:\n",
    "            print(\"âœ… CORRECT: 1536 dimensions!\")\n",
    "            print(f\"âœ… First 5 values: {sample_array[:5]}\")\n",
    "        else:\n",
    "            print(f\"âŒ WRONG: {len(sample_array)} dimensions (should be 1536)\")\n",
    "    \n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Semantic search FIXED - using true cosine similarity <=> (-1 to 1)\n"
     ]
    }
   ],
   "source": [
    "def semantic_search(query_text: str, top_k: int = 5) -> list:\n",
    "    \"\"\"\n",
    "    Semantic search menggunakan pgvector cosine similarity (<=>)\n",
    "    Using true cosine similarity, not distance\n",
    "    Returns: List of (content, filename, page_number, similarity_score)\n",
    "    \"\"\"\n",
    "    db = SessionLocal()\n",
    "    try:\n",
    "        # Generate query embedding\n",
    "        query_embedding = generate_embedding(query_text)\n",
    "\n",
    "        # Convert to pgvector array format: '[0.1, 0.2, ..., 1.5]'\n",
    "        embedding_str = '[' + ','.join(str(float(v)) for v in query_embedding) + ']'\n",
    "\n",
    "        # Use <=> operator for cosine similarity (range -1 to 1, where 1 is most similar)\n",
    "        result = db.execute(sql_text(f\"\"\"\n",
    "            SELECT\n",
    "                dc.content,\n",
    "                d.original_filename,\n",
    "                dc.page_number,\n",
    "                dc.embedding <=> '{embedding_str}'::vector as similarity_score\n",
    "            FROM document_chunk dc\n",
    "            JOIN document d ON dc.document_id = d.id\n",
    "            ORDER BY dc.embedding <=> '{embedding_str}'::vector DESC\n",
    "            LIMIT :limit\n",
    "        \"\"\"), {\"limit\": top_k})\n",
    "\n",
    "        results = []\n",
    "        for row in result.fetchall():\n",
    "            content, filename, page_num, similarity = row\n",
    "            results.append({\n",
    "                \"content\": content,\n",
    "                \"filename\": filename,\n",
    "                \"page\": page_num,\n",
    "                \"similarity\": float(similarity) if similarity else 0.0\n",
    "            })\n",
    "\n",
    "        return results\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "print(\"âœ… Semantic search FIXED - using true cosine similarity <=> (-1 to 1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Custom Query: \"automatic speech processing\"\n",
      "\n",
      "================================================================================\n",
      "Found 5 relevant chunks:\n",
      "\n",
      "  [1] ğŸ“„ H03 - NAT & OSPF.pdf\n",
      "      ğŸ“ Page 9\n",
      "      â­ Similarity: 0.8956\n",
      "      ğŸ“ CSCM603154 â€“ CSIM603154 Jaringan Komputer â€“ Jaringan Komunikasi Data Semester Gasal 2025/2026 Adapun ketentuan yang berlaku pada jaringan ini, yaitu: â€¢ Jumlah End-Device: Pada Divisi R&D terdapat 110 ...\n",
      "\n",
      "  [2] ğŸ“„ UTS_RPL_2425[1].pdf\n",
      "      ğŸ“ Page 6\n",
      "      â­ Similarity: 0.8709\n",
      "      ğŸ“ (3 Bangkuâ€¢. No. to Kelas: Adi NPM: Nama: penosjhat I a b le k o t a n i d a y p e o m k i m k ( a P h A os ). i D sw ot y a a a b d n i o m g s e b m n i e n y m g a o i n l n i g k . T i d n i i a s ...\n",
      "\n",
      "  [3] ğŸ“„ UTS_RPL_2425[1].pdf\n",
      "      ğŸ“ Page 2\n",
      "      â­ Similarity: 0.8670\n",
      "      ğŸ“ NPM: Nama: Kelas: No. Bangku: Perhatikan requirements pada aplikasi e-commerce berikut: \"Ketika pembeli melakukan checkout, maka pembeli harus membayar total harga termasuk ongkos kirim. Total harga v...\n",
      "\n",
      "  [4] ğŸ“„ H03 - NAT & OSPF.pdf\n",
      "      ğŸ“ Page 15\n",
      "      â­ Similarity: 0.8670\n",
      "      ğŸ“ CSCM603154 â€“ CSIM603154 Jaringan Komputer â€“ Jaringan Komunikasi Data Semester Gasal 2025/2026 [8 Poin] Topologi Pada bagian ini, lanjutkan topologi sebelumya menjadi topologi seperti gambar di atas. L...\n",
      "\n",
      "  [5] ğŸ“„ H03 - NAT & OSPF.pdf\n",
      "      ğŸ“ Page 11\n",
      "      â­ Similarity: 0.8653\n",
      "      ğŸ“ CSCM603154 â€“ CSIM603154 Jaringan Komputer â€“ Jaringan Komunikasi Data Semester Gasal 2025/2026 Alokasi IP Adapun network address yang digunakan oleh jaringan perusahaan Tech Innovators yaitu 192.168.0....\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "YOUR_QUERY = \"\"\"automatic speech processing\"\"\"  # â† Change this to test different queries\n",
    "\n",
    "print(f\"ğŸ¯ Custom Query: \\\"{YOUR_QUERY}\\\"\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = semantic_search(YOUR_QUERY, top_k=5)\n",
    "\n",
    "if results:\n",
    "    print(f\"Found {len(results)} relevant chunks:\\n\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"  [{i}] ğŸ“„ {result['filename']}\")\n",
    "        print(f\"      ğŸ“ Page {result['page']}\")\n",
    "        print(f\"      â­ Similarity: {result['similarity']:.4f}\")\n",
    "        print(f\"      ğŸ“ {result['content'][:200]}...\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"âŒ No relevant chunks found for this query\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
