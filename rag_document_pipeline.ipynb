{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06098ff5",
   "metadata": {},
   "source": [
    "# üöÄ RAG Document Ingestion Pipeline\n",
    "## From file_to_ingest Folder ‚Üí Storage ‚Üí PostgreSQL with pgvector\n",
    "\n",
    "**Pipeline:**\n",
    "1. Scan `file_to_ingest/` folder\n",
    "2. Upload files to storage\n",
    "3. Extract text from PDF (dengan page tracking)\n",
    "4. Chunk text (dengan overlap)\n",
    "5. Generate embeddings (OpenAI 1536-dim)\n",
    "6. Insert chunks ke PostgreSQL pgvector\n",
    "7. Test semantic search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c421c7",
   "metadata": {},
   "source": [
    "## 0Ô∏è‚É£ Setup - Database Connection & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39ab75e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATABASE CONNECTION TEST\n",
      "================================================================================\n",
      "Connected to system-llm-postgres-local\n",
      "Version: PostgreSQL 15.15 on x86_64-pc-linux-musl\n",
      "\n",
      "All imports loaded successfully\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import pdfplumber\n",
    "import io\n",
    "import re\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env.local explicitly (for Jupyter/local development)\n",
    "load_dotenv('.env.local')\n",
    "\n",
    "# ============================================================================\n",
    "# üîå DATABASE CONNECTION (via docker-compose exec)\n",
    "# ============================================================================\n",
    "\n",
    "class DatabaseConnection:\n",
    "    \"\"\"Docker PostgreSQL connection via docker-compose exec with stdin\"\"\"\n",
    "    def __init__(self, working_dir: str = \".\"):\n",
    "        self.working_dir = working_dir\n",
    "        self.container_name = \"system-llm-postgres-local\"\n",
    "        self.user = \"llm_user\"\n",
    "        self.db = \"system_llm\"\n",
    "\n",
    "    def execute_sql(self, query: str, fetch: bool = False) -> str:\n",
    "        \"\"\"Execute SQL via docker-compose exec with stdin (handles long queries)\"\"\"\n",
    "        cmd = [\n",
    "            \"docker-compose\", \"-f\", \"docker-compose.local.yml\",\n",
    "            \"exec\", \"-T\", \"postgres\",\n",
    "            \"psql\", \"-U\", self.user, \"-d\", self.db,\n",
    "        ]\n",
    "        \n",
    "        if fetch:\n",
    "            cmd.append(\"-t\")\n",
    "\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                cmd,\n",
    "                input=query,\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                encoding='utf-8',\n",
    "                errors='replace',\n",
    "                cwd=self.working_dir\n",
    "            )\n",
    "\n",
    "            if result.returncode != 0:\n",
    "                error_msg = result.stderr.strip() if result.stderr else \"Unknown error\"\n",
    "                raise Exception(f\"SQL Error: {error_msg}\")\n",
    "\n",
    "            return result.stdout.strip() if fetch else \"\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Database Error: {e}\")\n",
    "            raise\n",
    "\n",
    "    def test_connection(self) -> bool:\n",
    "        \"\"\"Test database connection\"\"\"\n",
    "        try:\n",
    "            version = self.execute_sql(\"SELECT version();\", fetch=True)\n",
    "            if version:\n",
    "                db_version = version.split(',')[0].strip()\n",
    "                print(f\"Connected to {self.container_name}\")\n",
    "                print(f\"Version: {db_version}\\n\")\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            print(f\"Connection failed: {e}\\n\")\n",
    "            return False\n",
    "\n",
    "# Initialize database\n",
    "db = DatabaseConnection(working_dir=\".\")\n",
    "print(\"=\" * 80)\n",
    "print(\"DATABASE CONNECTION TEST\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not db.test_connection():\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"All imports loaded successfully\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1699394",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Storage Setup - Local File Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50c5b19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Local storage initialized\n",
      "   Path: c:\\Users\\pcgsa\\Downloads\\system-llm\\system-llm-backend\\storage\\uploads\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üì¶ LOCAL FILE STORAGE\n",
    "# ============================================================================\n",
    "\n",
    "class LocalFileStorage:\n",
    "    \"\"\"Manage local file storage for uploaded PDFs\"\"\"\n",
    "    def __init__(self, base_path=\"storage/uploads\"):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.base_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def put(self, file_id: str, content: bytes) -> str:\n",
    "        \"\"\"Save file to storage\"\"\"\n",
    "        path = self.base_path / f\"{file_id}.pdf\"\n",
    "        path.write_bytes(content)\n",
    "        return file_id\n",
    "    \n",
    "    def get(self, file_id: str) -> bytes:\n",
    "        \"\"\"Retrieve file from storage\"\"\"\n",
    "        path = self.base_path / f\"{file_id}.pdf\"\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"File not found: {file_id}\")\n",
    "        return path.read_bytes()\n",
    "\n",
    "storage = LocalFileStorage()\n",
    "print(\"‚úÖ Local storage initialized\")\n",
    "print(f\"   Path: {storage.base_path.absolute()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef86b774",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Discover Files - Scan file_to_ingest Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47cf172f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Folder file_to_ingest sudah ada\n",
      "================================================================================\n",
      "üìÅ Scanning folder: c:\\Users\\pcgsa\\Downloads\\system-llm\\system-llm-backend\\file_to_ingest\n",
      "================================================================================\n",
      "\n",
      "Found 2 PDF file(s):\n",
      "\n",
      "  [1] 4b tts-id v2.pdf\n",
      "      Size: 1,870,838 bytes\n",
      "\n",
      "  [2] 5a dialog-systems-en v2.pdf\n",
      "      Size: 2,484,075 bytes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# ============================================================================\n",
    "# üìÅ CHECK & SCAN file_to_ingest FOLDER\n",
    "# ============================================================================\n",
    "\n",
    "ingest_folder = Path(\"file_to_ingest\")\n",
    "\n",
    "# Cek apakah folder sudah ada\n",
    "if not ingest_folder.exists():\n",
    "    print(\"üìÇ Folder file_to_ingest belum ada, membuat folder...\")\n",
    "    ingest_folder.mkdir(parents=True)\n",
    "else:\n",
    "    print(\"üìÇ Folder file_to_ingest sudah ada\")\n",
    "\n",
    "pdf_files = sorted(ingest_folder.glob(\"*.pdf\"))\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"üìÅ Scanning folder: {ingest_folder.absolute()}\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "if pdf_files:\n",
    "    print(f\"Found {len(pdf_files)} PDF file(s):\\n\")\n",
    "    for i, file_path in enumerate(pdf_files, 1):\n",
    "        file_size = file_path.stat().st_size\n",
    "        print(f\"  [{i}] {file_path.name}\")\n",
    "        print(f\"      Size: {file_size:,} bytes\\n\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No PDF files found in file_to_ingest folder\")\n",
    "    print(\"\\nüìù Please add PDF files to: file_to_ingest/\")\n",
    "    print(\"   Then run the next cells to process them.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a745fed3",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Select Files to Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8e1ab5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Selected files to ingest:\n",
      "\n",
      "  ‚úÖ [1] 4b tts-id v2.pdf\n",
      "  ‚úÖ [2] 5a dialog-systems-en v2.pdf\n",
      "\n",
      "‚úÖ Total files selected: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üìã SELECT FILES TO PROCESS\n",
    "# ============================================================================\n",
    "\n",
    "# MODIFY THIS: Change which files to process\n",
    "# Example: [1, 2] to process first and second file\n",
    "# Example: list(range(1, len(pdf_files) + 1)) to process all\n",
    "file_indices = list(range(1, len(pdf_files) + 1))  # Process ALL by default\n",
    "\n",
    "selected_files = []\n",
    "\n",
    "if pdf_files:\n",
    "    print(\"üìã Selected files to ingest:\\n\")\n",
    "    for idx in file_indices:\n",
    "        if 1 <= idx <= len(pdf_files):\n",
    "            file_path = pdf_files[idx - 1]\n",
    "            selected_files.append(file_path)\n",
    "            print(f\"  ‚úÖ [{idx}] {file_path.name}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Total files selected: {len(selected_files)}\\n\")\n",
    "else:\n",
    "    print(\"‚ùå No PDF files available to select\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c1d560",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Upload Files & Create DB Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unpsctog7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Clearing old data...\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Deleted all document_chunk records\n",
      "‚úÖ Deleted all document records\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üóëÔ∏è OPTIONAL: CLEAR OLD DATA (Run this first if re-ingesting)\n",
    "# ============================================================================\n",
    "\n",
    "CLEAR_OLD_DATA = False  # WARNING!!! Only set to True if you want to clear all old chunks documents before ingesting\n",
    "\n",
    "if CLEAR_OLD_DATA:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Clearing old data...\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "    \n",
    "    try:\n",
    "        delete_chunks = \"DELETE FROM document_chunk;\"\n",
    "        db.execute_sql(delete_chunks)\n",
    "        print(\"‚úÖ Deleted all document_chunk records\")\n",
    "        \n",
    "        delete_docs = \"DELETE FROM document;\"\n",
    "        db.execute_sql(delete_docs)\n",
    "        print(\"‚úÖ Deleted all document records\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Warning: {e}\\n\")\n",
    "else:\n",
    "    print(\"üí° To clear old data, set CLEAR_OLD_DATA = True at the top of this cell\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7814ff64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Uploading 2 file(s) to storage\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK [1] 4b tts-id v2.pdf\n",
      "      Size: 1,870,838 bytes\n",
      "      Storage ID: 0e3cbf88-14d1-4cd1-99d7-2cd85903ac8e\n",
      "      DB ID: 228e7029...\n",
      "\n",
      "  OK [2] 5a dialog-systems-en v2.pdf\n",
      "      Size: 2,484,075 bytes\n",
      "      Storage ID: eef27987-e3ce-4fe3-b209-afa2e02f4275\n",
      "      DB ID: e5ecd1a2...\n",
      "\n",
      "================================================================================\n",
      "SUCCESS: 2/2 file(s) uploaded\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üì§ UPLOAD FILES TO STORAGE & CREATE DB RECORDS\n",
    "# ============================================================================\n",
    "\n",
    "uploaded_documents = []\n",
    "\n",
    "if selected_files:\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Uploading {len(selected_files)} file(s) to storage\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "    \n",
    "    # Get a valid user_id from database (or use hardcoded default)\n",
    "    # For now, use first user in database\n",
    "    user_query = \"SELECT id FROM \\\"user\\\" LIMIT 1;\"\n",
    "    user_result = db.execute_sql(user_query, fetch=True)\n",
    "    \n",
    "    if user_result and user_result.strip():\n",
    "        user_id = user_result.strip()\n",
    "    else:\n",
    "        # Create default user if none exists\n",
    "        default_user_id = str(uuid.uuid4())\n",
    "        user_insert = f\"\"\"\n",
    "        INSERT INTO \\\"user\\\" (id, email, password_hash, full_name, role)\n",
    "        VALUES ('{default_user_id}', 'system@example.com', 'hash', 'System User', 'ADMIN');\n",
    "        \"\"\"\n",
    "        try:\n",
    "            db.execute_sql(user_insert)\n",
    "            user_id = default_user_id\n",
    "        except:\n",
    "            user_id = \"00000000-0000-0000-0000-000000000000\"\n",
    "    \n",
    "    for file_idx, file_path in enumerate(selected_files, 1):\n",
    "        try:\n",
    "            # Read file\n",
    "            file_content = file_path.read_bytes()\n",
    "            file_size = len(file_content)\n",
    "            \n",
    "            # Generate storage filename\n",
    "            storage_filename = str(uuid.uuid4())\n",
    "            file_path_str = f\"storage/uploads/{storage_filename}.pdf\"\n",
    "            \n",
    "            # Upload to storage\n",
    "            storage.put(storage_filename, file_content)\n",
    "            \n",
    "            # Create database record\n",
    "            db_id = str(uuid.uuid4())\n",
    "            \n",
    "            insert_query = f\"\"\"\n",
    "            INSERT INTO document (id, user_id, original_filename, filename, file_path, file_size, status, mime_type)\n",
    "            VALUES ('{db_id}', '{user_id}', '{file_path.name}', '{storage_filename}.pdf', '{file_path_str}', {file_size}, 'UPLOADED', 'application/pdf');\n",
    "            \"\"\"\n",
    "            \n",
    "            db.execute_sql(insert_query)\n",
    "            \n",
    "            uploaded_documents.append({\n",
    "                \"index\": file_idx,\n",
    "                \"db_id\": db_id,\n",
    "                \"storage_filename\": storage_filename,\n",
    "                \"original_filename\": file_path.name,\n",
    "                \"file_size\": file_size\n",
    "            })\n",
    "            \n",
    "            print(f\"  OK [{file_idx}] {file_path.name}\")\n",
    "            print(f\"      Size: {file_size:,} bytes\")\n",
    "            print(f\"      Storage ID: {storage_filename}\")\n",
    "            print(f\"      DB ID: {db_id[:8]}...\\n\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR [{file_idx}] {file_path.name}: {e}\\n\")\n",
    "    \n",
    "    if uploaded_documents:\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"SUCCESS: {len(uploaded_documents)}/{len(selected_files)} file(s) uploaded\")\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "    else:\n",
    "        print(\"FAILED: No files uploaded\\n\")\n",
    "else:\n",
    "    print(\"No files selected for upload\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6ce77d",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Extract Text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "botgcffkdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF extraction function loaded\n",
      "\n",
      "================================================================================\n",
      "Extracting text from 2 document(s)\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot set gray non-stroke color because /'Paint18' is an invalid float value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK 4b tts-id v2.pdf\n",
      "     Pages: 61, Chars: 19,344\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot set gray non-stroke color because /'P56' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P66' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P133' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P141' is an invalid float value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK 5a dialog-systems-en v2.pdf\n",
      "     Pages: 87, Chars: 27,813\n",
      "\n",
      "================================================================================\n",
      "SUCCESS: Extracted 2 document(s)\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXTRACT TEXT FROM PDF\n",
    "# ============================================================================\n",
    "\n",
    "def extract_text_from_pdf(pdf_bytes: bytes) -> dict:\n",
    "    \"\"\"Extract text from PDF with page tracking\"\"\"\n",
    "    pages_text = {}\n",
    "    try:\n",
    "        with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:\n",
    "            for page_num, page in enumerate(pdf.pages, 1):\n",
    "                extracted = page.extract_text()\n",
    "                if extracted and extracted.strip():\n",
    "                    pages_text[page_num] = extracted\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        raise\n",
    "    return pages_text\n",
    "\n",
    "print(\"PDF extraction function loaded\\n\")\n",
    "\n",
    "# Extract from uploaded documents\n",
    "extracted_texts = {}\n",
    "\n",
    "if uploaded_documents:\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Extracting text from {len(uploaded_documents)} document(s)\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "    \n",
    "    for doc in uploaded_documents:\n",
    "        db_id = doc[\"db_id\"]\n",
    "        storage_filename = doc[\"storage_filename\"]\n",
    "        filename = doc[\"original_filename\"]\n",
    "        \n",
    "        try:\n",
    "            pdf_bytes = storage.get(storage_filename)\n",
    "            pages_text = extract_text_from_pdf(pdf_bytes)\n",
    "            \n",
    "            if pages_text:\n",
    "                extracted_texts[db_id] = pages_text\n",
    "                total_chars = sum(len(t) for t in pages_text.values())\n",
    "                print(f\"  OK {filename}\")\n",
    "                print(f\"     Pages: {len(pages_text)}, Chars: {total_chars:,}\\n\")\n",
    "            else:\n",
    "                print(f\"  SKIP {filename}: No text extracted\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR {filename}: {e}\\n\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"SUCCESS: Extracted {len(extracted_texts)} document(s)\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "else:\n",
    "    print(\"No documents to extract\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98412aa0",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Chunk Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15pkjl190lm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CHUNK TEXT - FIXED VERSION (Cross-page chunking)\n",
    "# ============================================================================\n",
    "\n",
    "def chunk_text_with_pages(\n",
    "    pages_text: Dict[int, str],\n",
    "    chunk_size: int = 500,\n",
    "    overlap: int = 50\n",
    ") -> List[Tuple[str, int, int]]:\n",
    "    \"\"\"\n",
    "    Chunk text globally (not per-page) while tracking page numbers.\n",
    "    Returns: List of (chunk_content, start_page, end_page)\n",
    "\n",
    "    Algorithm:\n",
    "    1. Combine all pages into one text stream\n",
    "    2. Split by sentences\n",
    "    3. Build chunks from sentences (each chunk can span multiple pages)\n",
    "    4. Track which pages each chunk touches\n",
    "\n",
    "    Example:\n",
    "    - If chunk spans pages 5-7, it means content from those pages\n",
    "    - Overlap ensures context at chunk boundaries\n",
    "    \"\"\"\n",
    "    chunks_with_pages = []\n",
    "\n",
    "    # Convert to list of (sentence, page_number) tuples\n",
    "    all_sentences = []\n",
    "    for page_num in sorted(pages_text.keys()):\n",
    "        page_content = pages_text[page_num]\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', page_content)\n",
    "        for sentence in sentences:\n",
    "            if sentence.strip():\n",
    "                all_sentences.append((sentence, page_num))\n",
    "\n",
    "    if not all_sentences:\n",
    "        return chunks_with_pages\n",
    "\n",
    "    # Build chunks globally\n",
    "    current_chunk = []\n",
    "    current_pages = set()\n",
    "    current_size = 0\n",
    "\n",
    "    for sentence, page_num in all_sentences:\n",
    "        words = sentence.split()\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        # If adding this sentence exceeds chunk_size AND we have content, save chunk\n",
    "        if current_size + len(words) > chunk_size and current_chunk:\n",
    "            chunk_content = ' '.join(current_chunk)\n",
    "            start_page = min(current_pages)\n",
    "            end_page = max(current_pages)\n",
    "            chunks_with_pages.append((chunk_content, start_page, end_page))\n",
    "\n",
    "            # OVERLAP: Keep last N words (not last N%)\n",
    "            # This ensures next chunk starts with context from previous chunk\n",
    "            overlap_words = current_chunk[-overlap:] if len(current_chunk) > overlap else current_chunk\n",
    "            current_chunk = overlap_words\n",
    "            current_size = len(' '.join(current_chunk).split())\n",
    "            current_pages = {page_num}\n",
    "\n",
    "        current_chunk.extend(words)\n",
    "        current_pages.add(page_num)\n",
    "        current_size += len(words)\n",
    "\n",
    "    # Save remaining chunk\n",
    "    if current_chunk:\n",
    "        chunk_content = ' '.join(current_chunk)\n",
    "        start_page = min(current_pages)\n",
    "        end_page = max(current_pages)\n",
    "        chunks_with_pages.append((chunk_content, start_page, end_page))\n",
    "\n",
    "    return chunks_with_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "wu2gdqek3vf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text chunking function loaded (GLOBAL CHUNKING - spans multiple pages)\n",
      "\n",
      "================================================================================\n",
      "Creating chunks for 2 document(s)\n",
      "================================================================================\n",
      "\n",
      "  üìÑ 4b tts-id v2.pdf\n",
      "     Total Pages: 61\n",
      "     Total Chunks: 7\n",
      "     Single-page chunks: 0\n",
      "     Multi-page chunks: 7\n",
      "     Avg words/chunk: 455\n",
      "\n",
      "  üìÑ 5a dialog-systems-en v2.pdf\n",
      "     Total Pages: 87\n",
      "     Total Chunks: 11\n",
      "     Single-page chunks: 0\n",
      "     Multi-page chunks: 11\n",
      "     Avg words/chunk: 457\n",
      "\n",
      "================================================================================\n",
      "SUCCESS: Created 18 chunks\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Text chunking function loaded (GLOBAL CHUNKING - spans multiple pages)\\n\")\n",
    "\n",
    "# Create chunks\n",
    "chunks_by_document = {}\n",
    "\n",
    "if extracted_texts:\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Creating chunks for {len(extracted_texts)} document(s)\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "    \n",
    "    for doc_id, pages_text in extracted_texts.items():\n",
    "        chunks_with_pages = chunk_text_with_pages(pages_text, chunk_size=500, overlap=50)\n",
    "        chunks_by_document[doc_id] = chunks_with_pages\n",
    "        \n",
    "        doc = next((d for d in uploaded_documents if d[\"db_id\"] == doc_id), None)\n",
    "        if doc:\n",
    "            print(f\"  üìÑ {doc['original_filename']}\")\n",
    "            print(f\"     Total Pages: {len(pages_text)}\")\n",
    "            print(f\"     Total Chunks: {len(chunks_with_pages)}\")\n",
    "            \n",
    "            # Analyze chunk distribution\n",
    "            single_page = sum(1 for _, start, end in chunks_with_pages if start == end)\n",
    "            multi_page = sum(1 for _, start, end in chunks_with_pages if start != end)\n",
    "            \n",
    "            print(f\"     Single-page chunks: {single_page}\")\n",
    "            print(f\"     Multi-page chunks: {multi_page}\")\n",
    "            \n",
    "            if chunks_with_pages:\n",
    "                avg_words = sum(len(c[0].split()) for c in chunks_with_pages) / len(chunks_with_pages)\n",
    "                print(f\"     Avg words/chunk: {avg_words:.0f}\\n\")\n",
    "    \n",
    "    total = sum(len(c) for c in chunks_by_document.values())\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"SUCCESS: Created {total} chunks\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "else:\n",
    "    print(\"No text to chunk\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "xdndmej2k2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI embedding function loaded (1536 dimensions)\n",
      "Using API key: sk-proj-pH9p6un0rQCs...\n",
      "\n",
      "Testing embedding generation...\n",
      "  OK: Generated 1536-dimensional embedding\n",
      "      First 5 values: [-0.03277304023504257, 0.051569681614637375, 0.02663818560540676, -0.023709138855338097, -0.023732202127575874]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# GENERATE EMBEDDINGS (OpenAI)\n",
    "# ============================================================================\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"ERROR: OPENAI_API_KEY not set in .env.local\")\n",
    "    print(\"Make sure .env.local has: OPENAI_API_KEY=sk-proj-...\")\n",
    "    sys.exit(1)\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def generate_embedding(text: str) -> List[float]:\n",
    "    \"\"\"Generate 1536-dimensional embedding\"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "print(\"OpenAI embedding function loaded (1536 dimensions)\")\n",
    "print(f\"Using API key: {OPENAI_API_KEY[:20]}...\\n\")\n",
    "\n",
    "if chunks_by_document:\n",
    "    print(\"Testing embedding generation...\")\n",
    "    \n",
    "    first_doc_id = list(chunks_by_document.keys())[0]\n",
    "    first_chunk = chunks_by_document[first_doc_id][0][0]\n",
    "    \n",
    "    try:\n",
    "        embedding = generate_embedding(first_chunk[:1000])\n",
    "        print(f\"  OK: Generated {len(embedding)}-dimensional embedding\")\n",
    "        print(f\"      First 5 values: {embedding[:5]}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: {e}\\n\")\n",
    "        sys.exit(1)\n",
    "else:\n",
    "    print(\"No chunks for embedding test\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a726de29",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Insert Chunks to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0zie33e01xa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert function loaded (handles multi-page chunks)\n",
      "\n",
      "================================================================================\n",
      "Starting ingestion for 2 document(s)\n",
      "================================================================================\n",
      "\n",
      "[1/2] 4b tts-id v2.pdf\n",
      "  Status: PROCESSING\n",
      "  Inserting 7 chunks...\n",
      "  Status: PROCESSED\n",
      "\n",
      "[2/2] 5a dialog-systems-en v2.pdf\n",
      "  Status: PROCESSING\n",
      "  Inserting 11 chunks...\n",
      "     Progress: 10/11\n",
      "  Status: PROCESSED\n",
      "\n",
      "================================================================================\n",
      "INGESTION COMPLETE!\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# INSERT CHUNKS TO POSTGRESQL\n",
    "# ============================================================================\n",
    "\n",
    "def insert_chunks_to_db(document_id: str, chunks_with_pages: List[Tuple[str, int, int]]):\n",
    "    \"\"\"\n",
    "    Insert chunks with embeddings to PostgreSQL.\n",
    "    Now supports chunks spanning multiple pages.\n",
    "    \n",
    "    For multi-page chunks:\n",
    "    - page_number stores the START page\n",
    "    - chunk_metadata stores {\"start_page\": X, \"end_page\": Y}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Update status to PROCESSING\n",
    "        status_query = f\"UPDATE document SET status = 'PROCESSING' WHERE id = '{document_id}';\"\n",
    "        db.execute_sql(status_query)\n",
    "        print(f\"  Status: PROCESSING\")\n",
    "        \n",
    "        # Insert chunks\n",
    "        print(f\"  Inserting {len(chunks_with_pages)} chunks...\")\n",
    "        \n",
    "        for idx, (chunk_content, start_page, end_page) in enumerate(chunks_with_pages):\n",
    "            # Generate embedding\n",
    "            embedding = generate_embedding(chunk_content)\n",
    "            embedding_json = json.dumps(embedding)\n",
    "            \n",
    "            # Escape quotes for SQL\n",
    "            safe_content = chunk_content.replace(\"'\", \"''\")\n",
    "            safe_embedding = embedding_json.replace(\"'\", \"''\")\n",
    "            \n",
    "            # Store page range in metadata\n",
    "            metadata = json.dumps({\"start_page\": start_page, \"end_page\": end_page})\n",
    "            safe_metadata = metadata.replace(\"'\", \"''\")\n",
    "            \n",
    "            # Insert query\n",
    "            chunk_id = str(uuid.uuid4())\n",
    "            insert_query = f\"\"\"\n",
    "            INSERT INTO document_chunk\n",
    "            (id, document_id, chunk_index, content, page_number, embedding, chunk_metadata, created_at)\n",
    "            VALUES\n",
    "            ('{chunk_id}', '{document_id}', {idx}, '{safe_content}', {start_page}, '{safe_embedding}', '{safe_metadata}'::jsonb, now());\n",
    "            \"\"\"\n",
    "            \n",
    "            db.execute_sql(insert_query)\n",
    "            \n",
    "            if (idx + 1) % 10 == 0:\n",
    "                print(f\"     Progress: {idx + 1}/{len(chunks_with_pages)}\")\n",
    "        \n",
    "        # Update status to PROCESSED\n",
    "        processed_query = f\"UPDATE document SET status = 'PROCESSED', processed_at = now() WHERE id = '{document_id}';\"\n",
    "        db.execute_sql(processed_query)\n",
    "        print(f\"  Status: PROCESSED\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"Insert function loaded (handles multi-page chunks)\\n\")\n",
    "\n",
    "# Process all documents\n",
    "if uploaded_documents and chunks_by_document:\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Starting ingestion for {len(uploaded_documents)} document(s)\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "    \n",
    "    for doc_idx, doc in enumerate(uploaded_documents, 1):\n",
    "        document_id = doc[\"db_id\"]\n",
    "        filename = doc[\"original_filename\"]\n",
    "        \n",
    "        if document_id not in chunks_by_document:\n",
    "            print(f\"Skipping {filename} - no chunks\\n\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            chunks_with_pages = chunks_by_document[document_id]\n",
    "            print(f\"[{doc_idx}/{len(uploaded_documents)}] {filename}\")\n",
    "            insert_chunks_to_db(document_id, chunks_with_pages)\n",
    "            print()\n",
    "        except Exception as e:\n",
    "            print(f\"FAILED: {e}\\n\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"INGESTION COMPLETE!\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "else:\n",
    "    print(\"Missing prerequisites\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee731ae3",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Verification - Check Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ugw8wq8oan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VERIFICATION - Database Contents\n",
      "================================================================================\n",
      "\n",
      "Total chunks in database: 18\n",
      "\n",
      "Document Status:\n",
      "  No documents found\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VERIFICATION - Check Database\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"VERIFICATION - Database Contents\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "# Total chunks\n",
    "count_result = db.execute_sql(\"SELECT COUNT(*) FROM document_chunk;\", fetch=True).strip()\n",
    "total_chunks = int(count_result) if count_result and count_result.isdigit() else 0\n",
    "print(f\"Total chunks in database: {total_chunks}\\n\")\n",
    "\n",
    "# Document status\n",
    "status_query = \"\"\"\n",
    "SELECT d.original_filename, d.status, COUNT(dc.id) as chunk_count\n",
    "FROM document d LEFT JOIN document_chunk dc ON d.id = dc.document_id\n",
    "GROUP BY d.id, d.original_filename, d.status\n",
    "ORDER BY d.uploaded_at DESC;\n",
    "\"\"\"\n",
    "result = db.execute_sql(status_query, fetch=True)\n",
    "\n",
    "print(\"Document Status:\")\n",
    "if result:\n",
    "    for line in result.split('\\n'):\n",
    "        if line.strip() and '|' in line:\n",
    "            parts = line.split('|')\n",
    "            if len(parts) >= 3:\n",
    "                filename = parts[0].strip()\n",
    "                status = parts[1].strip()\n",
    "                chunk_count = parts[2].strip()\n",
    "                print(f\"  {filename}\")\n",
    "                print(f\"    Status: {status}, Chunks: {chunk_count}\\n\")\n",
    "else:\n",
    "    print(\"  No documents found\\n\")\n",
    "\n",
    "print(\"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0735c46b",
   "metadata": {},
   "source": [
    "## üîü Semantic Search Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "seclgt64n0h",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SEMANTIC SEARCH TEST\n",
      "================================================================================\n",
      "\n",
      "Query: ''Mengembangkan sistem text-to-speech berbasis ML Formalisasi Data evaluasi tugas Melatih Data pelatihan model Mengevalua Terapkan dan si model pantau''\n",
      "\n",
      "Found 5 results:\n",
      "\n",
      "  [1] 4b tts-id v2.pdf (page 1)\n",
      "      Similarity: 0.6858\n",
      "      Content: Sintesis Bentuk Gelombang Pemrosesan Bahasa Lisan Fakultas Ilmu Komputer Universitas Indonesia Semester Gasal 2024/2025 Referensi ‚ñ™ TTS Waveform Synth...\n",
      "\n",
      "  [2] 4b tts-id v2.pdf (page 11)\n",
      "      Similarity: 0.5994\n",
      "      Content: Ekspresi 1 2 3 4 5 o Seberapa baik intonasi sesuai dengan substansi ucapan? A/B testing ‚Ä¢ Menggunakan pilihan yang bersumber dari banyak orang untuk m...\n",
      "\n",
      "  [3] 4b tts-id v2.pdf (page 57)\n",
      "      Similarity: 0.5588\n",
      "      Content: unit (tidak seperti sintesis diphone) tidak dapat mengubah penekanan. ‚óã Seleksi unit memberikan hasil yang bagus (tetapi mungkin tidak sepenuhnya bena...\n",
      "\n",
      "  [4] 4b tts-id v2.pdf (page 27)\n",
      "      Similarity: 0.5305\n",
      "      Content: aksen Hasilkan: ‚óè Waveform 26 F0 Generation ‚óè Berdasarkan aturan ‚óè Menggunakan regresi linear atau machine learning ‚óè Beberapa batasan: ‚óã Berdasarkan ...\n",
      "\n",
      "  [5] 4b tts-id v2.pdf (page 49)\n",
      "      Similarity: 0.4771\n",
      "      Content: s-# 1 1 # UNITS # s-ih ih-k 2 2 k-s s-# 2 2 s-ih ih-k 3 3 Join Cost Pelatihan sintesis konkatenatif ‚Ä¢ Menggunakan estimasi bobot otomatis [Hunt dan Bl...\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SEMANTIC SEARCH TEST\n",
    "# ============================================================================\n",
    "\n",
    "def semantic_search(query_text: str, top_k: int = 5) -> list:\n",
    "    \"\"\"Semantic search using cosine similarity\"\"\"\n",
    "    try:\n",
    "        # Generate query embedding\n",
    "        query_embedding = np.array(generate_embedding(query_text))\n",
    "        \n",
    "        # Get chunks from database\n",
    "        search_query = \"\"\"\n",
    "        SELECT dc.content, d.original_filename, dc.page_number, dc.embedding\n",
    "        FROM document_chunk dc\n",
    "        JOIN document d ON dc.document_id = d.id\n",
    "        LIMIT 100;\n",
    "        \"\"\"\n",
    "        result_text = db.execute_sql(search_query, fetch=True)\n",
    "        \n",
    "        similarities = []\n",
    "        if result_text:\n",
    "            for line in result_text.split('\\n'):\n",
    "                if '|' in line:\n",
    "                    parts = line.split('|')\n",
    "                    if len(parts) >= 4:\n",
    "                        try:\n",
    "                            content = parts[0].strip()\n",
    "                            filename = parts[1].strip()\n",
    "                            page_num = int(parts[2].strip()) if parts[2].strip().isdigit() else 0\n",
    "                            embedding_json = parts[3].strip()\n",
    "                            \n",
    "                            chunk_embedding = np.array(json.loads(embedding_json))\n",
    "                            similarity = np.dot(query_embedding, chunk_embedding) / (\n",
    "                                np.linalg.norm(query_embedding) * np.linalg.norm(chunk_embedding) + 1e-10\n",
    "                            )\n",
    "                            similarities.append((content[:300], filename, page_num, float(similarity)))\n",
    "                        except:\n",
    "                            pass\n",
    "        \n",
    "        similarities.sort(key=lambda x: x[3], reverse=True)\n",
    "        return similarities[:top_k]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test search\n",
    "print(\"=\" * 80)\n",
    "print(\"SEMANTIC SEARCH TEST\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "QUERY = \"'Mengembangkan sistem text-to-speech berbasis ML Formalisasi Data evaluasi tugas Melatih Data pelatihan model Mengevalua Terapkan dan si model pantau'\"\n",
    "\n",
    "print(f\"Query: '{QUERY}'\\n\")\n",
    "\n",
    "results = semantic_search(QUERY, top_k=5)\n",
    "\n",
    "if results:\n",
    "    print(f\"Found {len(results)} results:\\n\")\n",
    "    for i, (content, filename, page, similarity) in enumerate(results, 1):\n",
    "        print(f\"  [{i}] {filename} (page {page})\")\n",
    "        print(f\"      Similarity: {similarity:.4f}\")\n",
    "        print(f\"      Content: {content[:150]}...\\n\")\n",
    "else:\n",
    "    print(\"No results found\\n\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
