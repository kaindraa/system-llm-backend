{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# üöÄ RAG Document Ingestion Pipeline - From Folder\n",
    "\n",
    "Step-by-step pipeline: Folder (file_to_ingest) ‚Üí Storage ‚Üí DB ‚Üí Text ‚Üí Chunks ‚Üí Embeddings ‚Üí pgvector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup Database Connection & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text as sql_text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import uuid\n",
    "\n",
    "load_dotenv(Path(\".env\"))\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "DATABASE_URL = os.getenv(\"DATABASE_URL\")\n",
    "if \"localhost:5432\" in DATABASE_URL:\n",
    "    DATABASE_URL = DATABASE_URL.replace(\"localhost:5432\", \"127.0.0.1:5433\")\n",
    "\n",
    "engine = create_engine(DATABASE_URL, pool_pre_ping=True, pool_size=5, pool_recycle=3600)\n",
    "SessionLocal = sessionmaker(bind=engine)\n",
    "\n",
    "print(\"‚úÖ Database engine created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    result = conn.execute(sql_text(\"SELECT current_database(), version()\"))\n",
    "    db_name, version = result.fetchone()\n",
    "    print(f\"‚úÖ Connected to: {db_name}\")\n",
    "    print(f\"‚úÖ PostgreSQL: {version.split(',')[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Setup Storage Provider & Create file_to_ingest Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalFileStorage:\n",
    "    def __init__(self, base_path=\"storage/uploads\"):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.base_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def put(self, file_id: str, content: bytes) -> str:\n",
    "        \"\"\"Save file to storage and return the file_id\"\"\"\n",
    "        path = self.base_path / f\"{file_id}.pdf\"\n",
    "        path.write_bytes(content)\n",
    "        return file_id\n",
    "    \n",
    "    def get(self, file_id: str) -> bytes:\n",
    "        path = self.base_path / f\"{file_id}.pdf\"\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"File not found: {file_id}\")\n",
    "        return path.read_bytes()\n",
    "\n",
    "class GCSStorageProvider:\n",
    "    def __init__(self, bucket_name: str, credentials_path: str = None, project_id: str = None):\n",
    "        from google.cloud import storage as gcs_storage\n",
    "        from google.oauth2 import service_account\n",
    "        \n",
    "        if bucket_name.startswith('gs://'):\n",
    "            bucket_name = bucket_name[5:]\n",
    "        \n",
    "        self.bucket_name = bucket_name\n",
    "        \n",
    "        if credentials_path and os.path.exists(credentials_path):\n",
    "            creds = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "            self.client = gcs_storage.Client(credentials=creds, project=project_id or creds.project_id)\n",
    "        else:\n",
    "            self.client = gcs_storage.Client(project=project_id)\n",
    "        \n",
    "        self.bucket = self.client.bucket(self.bucket_name)\n",
    "    \n",
    "    def put(self, file_id: str, content: bytes) -> str:\n",
    "        \"\"\"Upload file to GCS and return the file_id\"\"\"\n",
    "        blob = self.bucket.blob(f\"uploads/{file_id}.pdf\")\n",
    "        blob.upload_from_string(content)\n",
    "        return file_id\n",
    "    \n",
    "    def get(self, file_id: str) -> bytes:\n",
    "        blob = self.bucket.blob(f\"uploads/{file_id}.pdf\")\n",
    "        if not blob.exists():\n",
    "            raise FileNotFoundError(f\"File not found: {file_id}\")\n",
    "        return blob.download_as_bytes()\n",
    "\n",
    "# Initialize storage\n",
    "STORAGE_TYPE = os.getenv(\"STORAGE_TYPE\", \"local\").lower()\n",
    "if STORAGE_TYPE == \"gcs\":\n",
    "    storage = GCSStorageProvider(\n",
    "        os.getenv(\"GCS_BUCKET_NAME\"),\n",
    "        os.getenv(\"GCS_CREDENTIALS_PATH\"),\n",
    "        os.getenv(\"GCS_PROJECT_ID\")\n",
    "    )\n",
    "else:\n",
    "    storage = LocalFileStorage()\n",
    "\n",
    "print(f\"‚úÖ Storage provider: {storage.__class__.__name__}\")\n",
    "\n",
    "# Create file_to_ingest folder\n",
    "ingest_folder = Path(\"file_to_ingest\")\n",
    "ingest_folder.mkdir(exist_ok=True)\n",
    "print(f\"‚úÖ Created/verified folder: {ingest_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Discover & List Files in file_to_ingest Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan folder for PDF files\n",
    "pdf_files = list(ingest_folder.glob(\"*.pdf\"))\n",
    "other_files = [f for f in ingest_folder.glob(\"*\") if f.is_file() and f.suffix.lower() not in ['.pdf']]\n",
    "\n",
    "print(f\"üìÅ Scanning: {ingest_folder.absolute()}\\n\")\n",
    "print(f\"Found {len(pdf_files)} PDF file(s)\")\n",
    "\n",
    "if pdf_files:\n",
    "    print(\"\\nüìÑ PDF Files:\")\n",
    "    for i, file_path in enumerate(pdf_files, 1):\n",
    "        file_size = file_path.stat().st_size\n",
    "        print(f\"  [{i}] {file_path.name} ({file_size:,} bytes)\")\nelse:\n",
    "    print(\"\\n‚ö†Ô∏è  No PDF files found in file_to_ingest folder\")\n",
    "\n",
    "if other_files:\n",
    "    print(f\"\\n‚ö†Ô∏è  {len(other_files)} other file(s) (not PDF):\")\n",
    "    for f in other_files[:5]:\n",
    "        print(f\"  - {f.name}\")\n",
    "    if len(other_files) > 5:\n",
    "        print(f\"  ... and {len(other_files) - 5} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select files to ingest (as list of indices)\n",
    "file_indices = list(range(1, len(pdf_files) + 1))  # Change this to select specific files\n",
    "# Example: [1, 2] to ingest first and second file, or [1] for just the first\n",
    "\n",
    "selected_files = []\n",
    "\n",
    "if pdf_files:\n",
    "    print(f\"üìã Selected files to ingest:\\n\")\n",
    "    for idx in file_indices:\n",
    "        if 1 <= idx <= len(pdf_files):\n",
    "            file_path = pdf_files[idx - 1]\n",
    "            selected_files.append(file_path)\n",
    "            print(f\"  ‚úÖ [{idx}] {file_path.name}\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå [{idx}] Invalid index (out of range)\")\n",
    "    \n",
    "    if selected_files:\n",
    "        print(f\"\\n‚úÖ Total files to ingest: {len(selected_files)}\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå No valid files selected\")\nelse:\n",
    "    print(\"‚ùå No PDF files to select\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Upload Files to Storage & Create Document Records in DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "uploaded_documents = []  # Store document info {db_id, storage_filename, original_filename, file_size}\n",
    "\n",
    "if selected_files:\n",
    "    print(f\"üì§ Uploading {len(selected_files)} file(s) to storage & creating DB records...\\n\")\n",
    "    \n",
    "    for file_idx, file_path in enumerate(selected_files, 1):\n",
    "        try:\n",
    "            # Read file content\n",
    "            file_content = file_path.read_bytes()\n",
    "            file_size = len(file_content)\n",
    "            \n",
    "            # Generate unique storage filename\n",
    "            storage_filename = str(uuid.uuid4())\n",
    "            \n",
    "            # Upload to storage\n",
    "            storage.put(storage_filename, file_content)\n",
    "            \n",
    "            # Insert document record into database\n",
    "            db = SessionLocal()\n",
    "            try:\n",
    "                db.execute(sql_text(\"\"\"\n",
    "                    INSERT INTO document (id, original_filename, filename, file_size, status, created_at)\n",
    "                    VALUES (:id, :original_filename, :filename, :file_size, :status, :created_at)\n",
    "                \"\"\"), {\n",
    "                    \"id\": str(uuid.uuid4()),\n",
    "                    \"original_filename\": file_path.name,\n",
    "                    \"filename\": f\"{storage_filename}.pdf\",\n",
    "                    \"file_size\": file_size,\n",
    "                    \"status\": \"UPLOADED\",\n",
    "                    \"created_at\": datetime.utcnow()\n",
    "                })\n",
    "                db.commit()\n",
    "                \n",
    "                # Get the inserted document ID\n",
    "                result = db.execute(sql_text(\"\"\"\n",
    "                    SELECT id FROM document WHERE filename = :filename\n",
    "                \"\"\"), {\"filename\": f\"{storage_filename}.pdf\"})\n",
    "                document_id = result.scalar()\n",
    "                \n",
    "                uploaded_documents.append({\n",
    "                    \"index\": file_idx,\n",
    "                    \"db_id\": document_id,\n",
    "                    \"storage_filename\": storage_filename,\n",
    "                    \"original_filename\": file_path.name,\n",
    "                    \"file_size\": file_size\n",
    "                })\n",
    "                \n",
    "                print(f\"  ‚úÖ [{file_idx}] {file_path.name}\")\n",
    "                print(f\"     Size: {file_size:,} bytes\")\n",
    "                print(f\"     Storage ID: {storage_filename}\")\n",
    "                print(f\"     DB ID: {document_id}\\n\")\n",
    "            finally:\n",
    "                db.close()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå [{file_idx}] {file_path.name}: {e}\\n\")\n",
    "    \n",
    "    if uploaded_documents:\n",
    "        print(f\"‚úÖ Successfully uploaded {len(uploaded_documents)}/{len(selected_files)} file(s)\")\n",
    "    else:\n",
    "        print(f\"‚ùå No files uploaded successfully\")\nelse:\n",
    "    print(\"‚ö†Ô∏è  No files selected for upload\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Extract Text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import io\n",
    "\n",
    "def extract_text_from_pdf(pdf_bytes: bytes) -> dict:\n",
    "    \"\"\"\n",
    "    Extract text from PDF with page number tracking.\n",
    "    Returns: {page_number: full_text_for_page, ...}\n",
    "    \"\"\"\n",
    "    pages_text = {}\n",
    "    try:\n",
    "        with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:\n",
    "            for page_num, page in enumerate(pdf.pages, 1):  # Start from page 1\n",
    "                extracted = page.extract_text()\n",
    "                if extracted:\n",
    "                    pages_text[page_num] = extracted\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting PDF: {e}\")\n",
    "        raise\n",
    "    return pages_text\n",
    "\n",
    "print(\"‚úÖ PDF extraction function loaded (with page tracking)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_texts = {}  # Dictionary: {db_id: {page_num: text}}\n",
    "\n",
    "if uploaded_documents:\n",
    "    print(f\"üìÑ Extracting text from {len(uploaded_documents)} document(s)...\\n\")\n",
    "    \n",
    "    for doc in uploaded_documents:\n",
    "        db_id = doc[\"db_id\"]\n",
    "        storage_filename = doc[\"storage_filename\"]\n",
    "        filename = doc[\"original_filename\"]\n",
    "        \n",
    "        try:\n",
    "            # Retrieve file from storage\n",
    "            pdf_bytes = storage.get(storage_filename)\n",
    "            pages_text = extract_text_from_pdf(pdf_bytes)\n",
    "            extracted_texts[db_id] = pages_text\n",
    "            \n",
    "            total_chars = sum(len(t) for t in pages_text.values())\n",
    "            print(f\"  ‚úÖ {filename}: {len(pages_text)} pages, {total_chars} characters\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå {filename}: {e}\")\n",
    "    \n",
    "    if extracted_texts:\n",
    "        print(f\"\\n‚úÖ Extracted {len(extracted_texts)} document(s) with page tracking\")\n",
    "    else:\n",
    "        print(f\"‚ùå No documents extracted successfully\")\nelse:\n",
    "    print(\"‚ö†Ô∏è  No uploaded documents to extract\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Execute Text Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def chunk_text_with_pages(pages_text: Dict[int, str], chunk_size: int = 500, overlap: int = 50) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Chunk text while preserving page numbers.\n",
    "    Returns: [(chunk_content, page_number), ...]\n",
    "    \"\"\"\n",
    "    chunks_with_pages = []\n",
    "    \n",
    "    for page_num in sorted(pages_text.keys()):\n",
    "        page_content = pages_text[page_num]\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', page_content)\n",
    "        \n",
    "        current_chunk = []\n",
    "        current_size = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            words = sentence.split()\n",
    "            if current_size + len(words) > chunk_size and current_chunk:\n",
    "                chunk_content = ' '.join(current_chunk)\n",
    "                chunks_with_pages.append((chunk_content, page_num))\n",
    "                current_chunk = current_chunk[-int(overlap/10):]\n",
    "                current_size = len(' '.join(current_chunk).split())\n",
    "            \n",
    "            current_chunk.extend(words)\n",
    "            current_size += len(words)\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunk_content = ' '.join(current_chunk)\n",
    "            chunks_with_pages.append((chunk_content, page_num))\n",
    "    \n",
    "    return chunks_with_pages\n",
    "\n",
    "print(\"‚úÖ Text chunking function loaded (with page tracking)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_by_document = {}  # Dictionary: {db_id: [(content, page_num), ...]}\n",
    "\n",
    "if extracted_texts:\n",
    "    print(f\"üì¶ Creating chunks for {len(extracted_texts)} document(s)...\\n\")\n",
    "    \n",
    "    for doc_id, pages_text in extracted_texts.items():\n",
    "        chunks_with_pages = chunk_text_with_pages(pages_text, chunk_size=500)\n",
    "        chunks_by_document[doc_id] = chunks_with_pages\n",
    "        \n",
    "        # Get filename for display\n",
    "        doc = next((d for d in uploaded_documents if d[\"db_id\"] == doc_id), None)\n",
    "        if doc:\n",
    "            print(f\"  ‚úÖ {doc['original_filename']}: {len(chunks_with_pages)} chunks\")\n",
    "            if chunks_with_pages:\n",
    "                content, page_num = chunks_with_pages[0]\n",
    "                print(f\"     Sample: Page {page_num}: {content[:80]}...\")\n",
    "    \n",
    "    total_chunks = sum(len(c) for c in chunks_by_document.values())\n",
    "    print(f\"\\n‚úÖ Total chunks created: {total_chunks}\")\nelse:\n",
    "    print(\"‚ö†Ô∏è  No extracted text to chunk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Load OpenAI & Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY not set in .env\")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def generate_embedding(text: str) -> List[float]:\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    # Use full 1536 dimensions (updated to match database schema)\n",
    "    embedding = response.data[0].embedding\n",
    "    return embedding\n",
    "\n",
    "print(\"‚úÖ OpenAI embedding function loaded (1536 dimensions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if chunks_by_document:\n",
    "    print(\"üîó Generating sample embedding...\\n\")\n",
    "    \n",
    "    for doc_id, chunks_with_pages in list(chunks_by_document.items())[:1]:  # Just test first document\n",
    "        if chunks_with_pages:\n",
    "            # Get filename for display\n",
    "            doc = next((d for d in uploaded_documents if d[\"db_id\"] == doc_id), None)\n",
    "            if doc:\n",
    "                chunk_content, page_num = chunks_with_pages[0]\n",
    "                print(f\"  Testing: {doc['original_filename']} (Page {page_num})\")\n",
    "                embedding = generate_embedding(chunk_content)\n",
    "                \n",
    "                print(f\"  ‚úÖ Embedding generated\")\n",
    "                print(f\"     Dimension: {len(embedding)}\")\n",
    "                print(f\"     First 5 values: {embedding[:5]}\")\nelse:\n",
    "    print(\"‚ö†Ô∏è  No chunks to embed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Insert Chunks & Finalize Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.models.document_chunk import DocumentChunk\n",
    "\n",
    "def insert_chunks_and_finalize(document_id: str, chunks_with_pages: list):\n",
    "    \"\"\"\n",
    "    Insert chunks ke PostgreSQL dengan pgvector embeddings (1536 dimensions)\n",
    "    FIXED: Use SQLAlchemy ORM untuk handle vector type dengan benar\n",
    "    \"\"\"\n",
    "    db = SessionLocal()\n",
    "    try:\n",
    "        # 1. Update status to PROCESSING\n",
    "        db.execute(sql_text(\"\"\"\n",
    "            UPDATE document SET status = 'PROCESSING' WHERE id = :id\n",
    "        \"\"\"), {\"id\": document_id})\n",
    "        db.commit()\n",
    "        print(f\"  ‚úì Status: PROCESSING\")\n",
    "        \n",
    "        # 2. Generate embeddings and insert chunks\n",
    "        print(f\"  ‚úì Inserting {len(chunks_with_pages)} chunks...\")\n",
    "        for idx, (chunk_content, page_number) in enumerate(chunks_with_pages):\n",
    "            embedding = generate_embedding(chunk_content)  # Returns List[float]\n",
    "            \n",
    "            # Metadata JSON\n",
    "            chunk_metadata = {\n",
    "                \"page\": page_number,\n",
    "                \"chunk_sequence\": idx\n",
    "            }\n",
    "            \n",
    "            # ‚úÖ FIXED: Use ORM object directly\n",
    "            chunk = DocumentChunk(\n",
    "                document_id=document_id,\n",
    "                chunk_index=idx,\n",
    "                content=chunk_content,\n",
    "                page_number=page_number,\n",
    "                embedding=embedding,  # Pass as list - SQLAlchemy handles pgvector conversion\n",
    "                chunk_metadata=chunk_metadata\n",
    "            )\n",
    "            \n",
    "            db.add(chunk)\n",
    "            db.flush()  # Flush to ensure it's saved but don't commit yet\n",
    "            \n",
    "            if (idx + 1) % 5 == 0:\n",
    "                print(f\"     {idx + 1}/{len(chunks_with_pages)} inserted...\")\n",
    "        \n",
    "        db.commit()  # Commit all at once\n",
    "        print(f\"  ‚úì All {len(chunks_with_pages)} chunks inserted\")\n",
    "        \n",
    "        # 3. Update document status to PROCESSED\n",
    "        db.execute(sql_text(\"\"\"\n",
    "            UPDATE document SET status = 'PROCESSED', processed_at = :now WHERE id = :id\n",
    "        \"\"\"), {\"now\": datetime.utcnow(), \"id\": document_id})\n",
    "        db.commit()\n",
    "        print(f\"  ‚úì Status: PROCESSED\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        db.execute(sql_text(\"\"\"\n",
    "            UPDATE document SET status = 'FAILED' WHERE id = :id\n",
    "        \"\"\"), {\"id\": document_id})\n",
    "        db.commit()\n",
    "        print(f\"\\n‚ùå Error: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "print(\"‚úÖ Insert function loaded - using SQLAlchemy ORM with pgvector support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ EXECUTION - Process & Ingest All Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "if uploaded_documents and chunks_by_document:\n",
    "    print(f\"üöÄ Starting RAG ingestion for {len(uploaded_documents)} document(s)\\n\")\n",
    "    \n",
    "    for doc_idx, doc in enumerate(uploaded_documents, 1):\n",
    "        document_id = doc[\"db_id\"]\n",
    "        filename = doc[\"original_filename\"]\n",
    "        \n",
    "        if document_id not in chunks_by_document:\n",
    "            print(f\"‚ö†Ô∏è  [{doc_idx}] {filename} - Skipping (no chunks available)\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            chunks_with_pages = chunks_by_document[document_id]\n",
    "            print(f\"\\n[{doc_idx}/{len(uploaded_documents)}] Processing: {filename}\")\n",
    "            print(f\"    Chunks: {len(chunks_with_pages)}\")\n",
    "            insert_chunks_and_finalize(document_id, chunks_with_pages)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing document: {e}\\n\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\nelse:\n",
    "    print(\"‚ö†Ô∏è  Missing prerequisites\")\n",
    "    if not uploaded_documents:\n",
    "        print(\"   - Upload documents first (run Steps 3-4)\")\n",
    "    if not chunks_by_document:\n",
    "        print(\"   - Extract and chunk text first (run Steps 5-6)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## üîü Quick Verification - Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÖ Quick Embedding Verification\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def parse_vector_string(vector_str: str) -> list:\n",
    "    \"\"\"Parse PostgreSQL vector string to Python list\"\"\"\n",
    "    if not vector_str:\n",
    "        return []\n",
    "    # Remove brackets and split by comma\n",
    "    vector_str = vector_str.strip().strip('[]')\n",
    "    return [float(x.strip()) for x in vector_str.split(',') if x.strip()]\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    # Get sample embedding\n",
    "    result = conn.execute(sql_text(\"\"\"\n",
    "        SELECT COUNT(*) as total, \n",
    "               (SELECT embedding FROM document_chunk LIMIT 1) as sample_emb\n",
    "        FROM document_chunk\n",
    "    \"\"\")).fetchone()\n",
    "    \n",
    "    total, sample = result\n",
    "    \n",
    "    print(f\"Total chunks in database: {total}\")\n",
    "    \n",
    "    if sample:\n",
    "        # Parse vector string to list\n",
    "        if isinstance(sample, str):\n",
    "            sample_array = parse_vector_string(sample)\n",
    "        else:\n",
    "            sample_array = sample\n",
    "        \n",
    "        print(f\"Sample embedding raw: {sample[:50]}...\" if len(sample) > 50 else f\"Sample embedding raw: {sample}\")\n",
    "        print(f\"Sample embedding type (parsed): {type(sample_array).__name__}\")\n",
    "        print(f\"Sample embedding dimensions: {len(sample_array)}\")\n",
    "        \n",
    "        if len(sample_array) == 1536:\n",
    "            print(\"‚úÖ CORRECT: 1536 dimensions!\")\n",
    "            print(f\"‚úÖ First 5 values: {sample_array[:5]}\")\n",
    "        else:\n",
    "            print(f\"‚ùå WRONG: {len(sample_array)} dimensions (should be 1536)\")\n",
    "    \n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Semantic Search Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query_text: str, top_k: int = 5) -> list:\n",
    "    \"\"\"\n",
    "    Semantic search menggunakan pgvector cosine similarity (<=>)\n",
    "    Using true cosine similarity, not distance\n",
    "    Returns: List of (content, filename, page_number, similarity_score)\n",
    "    \"\"\"\n",
    "    db = SessionLocal()\n",
    "    try:\n",
    "        # Generate query embedding\n",
    "        query_embedding = generate_embedding(query_text)\n",
    "\n",
    "        # Convert to pgvector array format: '[0.1, 0.2, ..., 1.5]'\n",
    "        embedding_str = '[' + ','.join(str(float(v)) for v in query_embedding) + ']'\n",
    "\n",
    "        # Use <=> operator for cosine similarity (range -1 to 1, where 1 is most similar)\n",
    "        result = db.execute(sql_text(f\"\"\"\n",
    "            SELECT\n",
    "                dc.content,\n",
    "                d.original_filename,\n",
    "                dc.page_number,\n",
    "                dc.embedding <=> '{embedding_str}'::vector as similarity_score\n",
    "            FROM document_chunk dc\n",
    "            JOIN document d ON dc.document_id = d.id\n",
    "            ORDER BY dc.embedding <=> '{embedding_str}'::vector DESC\n",
    "            LIMIT :limit\n",
    "        \"\"\"), {\"limit\": top_k})\n",
    "\n",
    "        results = []\n",
    "        for row in result.fetchall():\n",
    "            content, filename, page_num, similarity = row\n",
    "            results.append({\n",
    "                \"content\": content,\n",
    "                \"filename\": filename,\n",
    "                \"page\": page_num,\n",
    "                \"similarity\": float(similarity) if similarity else 0.0\n",
    "            })\n",
    "\n",
    "        return results\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "print(\"‚úÖ Semantic search function loaded - using cosine similarity (<=>)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_QUERY = \"your query here\"  # ‚Üê Change this to test different queries\n",
    "\n",
    "print(f\"üéØ Custom Query: \\\"{YOUR_QUERY}\\\"\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = semantic_search(YOUR_QUERY, top_k=5)\n",
    "\n",
    "if results:\n",
    "    print(f\"Found {len(results)} relevant chunks:\\n\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"  [{i}] üìÑ {result['filename']}\")\n",
    "        print(f\"      üìç Page {result['page']}\")\n",
    "        print(f\"      ‚≠ê Similarity: {result['similarity']:.4f}\")\n",
    "        print(f\"      üìù {result['content'][:200]}...\")\n",
    "        print()\nelse:\n",
    "    print(\"‚ùå No relevant chunks found for this query\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}